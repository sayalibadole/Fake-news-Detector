{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayalibadole/Fake-news-Detector/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvl4f6hwnFl3"
      },
      "source": [
        "# **Final Project**\n",
        "\n",
        "**Fake News Detector By Christopher Lendechy and Sayali Badole**\n",
        "\n",
        "Date: 12th December, 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRDob8bYkGUu"
      },
      "source": [
        "## **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNOHzoG7oLN6"
      },
      "source": [
        "Fake news is false or misleading information that is typically used maliciously. In the last couple of years, fake news has become widespread issue across the internet, especially on social media sites. This type of news speads easily on social media due to how difficult it is to disguish fake news at a glance. \n",
        "\n",
        "This notebook contains several approaches to predict if a given news is fake or not. This “Fake News Detector” would identify if an article is trustworthy or not from its article title. Further, into the project, we would also attempt to identify if the news is likely to be fake based on its source and the number of re-tweets it gets.\n",
        "\n",
        "For our dataset, we are using a fake news dataset from Kaggle based on FakeNewsNet. In this dataset, every entry represents a tweet containing an article headline, a URL linking to an actual article, the source of the article, how many re-tweets it has, and if the news is fake.This dataset is appropriate for our project because it contains both fake and real news from a variety of different topics and sources. Additionally, all of the fake news articles were verified by Politifact and Gossipcop, both known for fact-checking sources online.\n",
        "\n",
        "Link to the dataset:  https://www.kaggle.com/datasets/algord/fake-news \n",
        "\n",
        "Creating a model that could help predict the legitimacy of a news article can be very valuable and can be used and integrated into any system for future use. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRhG0BD8IuId"
      },
      "source": [
        "## **Read the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTA7C05bovMI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import models, layers, Model, Input, callbacks\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
        "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras import backend as K \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "pd.options.display.max_colwidth = 100\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from pandas.core.ops.array_ops import na_logical_op\n",
        "from sklearn.model_selection import ParameterGrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkRiZzy8rATY",
        "outputId": "4db2815f-3833-4867-dddf-f2d23cac6479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir='/content/drive/MyDrive/FakeNewsNet.csv'\n",
        "df= pd.read_csv(data_dir,low_memory=False)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZIpEUrIoQ_a"
      },
      "source": [
        "## **Initial Exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74y90KOHJBei"
      },
      "source": [
        "Starting off with getting to know the data. Here are the first few rows of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "q9YUZQc_s-AQ",
        "outputId": "52e94290-3209-4d31-8a6f-37f0a0d1f10e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4c78d038-b9a6-418a-bb19-5c14fa333ff9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>news_url</th>\n",
              "      <th>source_domain</th>\n",
              "      <th>tweet_num</th>\n",
              "      <th>real</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kandi Burruss Explodes Over Rape Accusation on 'Real Housewives of Atlanta' Reunion (Video)</td>\n",
              "      <td>http://toofab.com/2017/05/08/real-housewives-atlanta-kandi-burruss-rape-phaedra-parks-porsha-wil...</td>\n",
              "      <td>toofab.com</td>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>People's Choice Awards 2018: The best red carpet looks</td>\n",
              "      <td>https://www.today.com/style/see-people-s-choice-awards-red-carpet-looks-t141832</td>\n",
              "      <td>www.today.com</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sophia Bush Sends Sweet Birthday Message to 'One Tree Hill' Co-Star Hilarie Burton: 'Breyton 4eva'</td>\n",
              "      <td>https://www.etonline.com/news/220806_sophia_bush_sends_sweet_birthday_message_to_one_tree_hill_c...</td>\n",
              "      <td>www.etonline.com</td>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Colombian singer Maluma sparks rumours of inappropriate relationship with AUNT</td>\n",
              "      <td>https://www.dailymail.co.uk/news/article-3365543/Colombian-music-star-sparks-rumours-inappropria...</td>\n",
              "      <td>www.dailymail.co.uk</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gossip Girl 10 Years Later: How Upper East Siders Shocked the World and Changed Pop Culture Forever</td>\n",
              "      <td>https://www.zerchoo.com/entertainment/gossip-girl-10-years-later-how-upper-east-siders-shocked-t...</td>\n",
              "      <td>www.zerchoo.com</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c78d038-b9a6-418a-bb19-5c14fa333ff9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c78d038-b9a6-418a-bb19-5c14fa333ff9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c78d038-b9a6-418a-bb19-5c14fa333ff9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                 title  \\\n",
              "0          Kandi Burruss Explodes Over Rape Accusation on 'Real Housewives of Atlanta' Reunion (Video)   \n",
              "1                                               People's Choice Awards 2018: The best red carpet looks   \n",
              "2   Sophia Bush Sends Sweet Birthday Message to 'One Tree Hill' Co-Star Hilarie Burton: 'Breyton 4eva'   \n",
              "3                       Colombian singer Maluma sparks rumours of inappropriate relationship with AUNT   \n",
              "4  Gossip Girl 10 Years Later: How Upper East Siders Shocked the World and Changed Pop Culture Forever   \n",
              "\n",
              "                                                                                              news_url  \\\n",
              "0  http://toofab.com/2017/05/08/real-housewives-atlanta-kandi-burruss-rape-phaedra-parks-porsha-wil...   \n",
              "1                      https://www.today.com/style/see-people-s-choice-awards-red-carpet-looks-t141832   \n",
              "2  https://www.etonline.com/news/220806_sophia_bush_sends_sweet_birthday_message_to_one_tree_hill_c...   \n",
              "3  https://www.dailymail.co.uk/news/article-3365543/Colombian-music-star-sparks-rumours-inappropria...   \n",
              "4  https://www.zerchoo.com/entertainment/gossip-girl-10-years-later-how-upper-east-siders-shocked-t...   \n",
              "\n",
              "         source_domain  tweet_num  real  \n",
              "0           toofab.com         42     1  \n",
              "1        www.today.com          0     1  \n",
              "2     www.etonline.com         63     1  \n",
              "3  www.dailymail.co.uk         20     1  \n",
              "4      www.zerchoo.com         38     1  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rdV0AGIJFSk"
      },
      "source": [
        "We can see that there are 4 different columns, the first few rows gives us an idea about the type of data in the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX03bWrRtH3i",
        "outputId": "ac48aaa4-1454-4066-c3a2-17e0eb34b703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 23196 entries, 0 to 23195\n",
            "Data columns (total 5 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   title          23196 non-null  object\n",
            " 1   news_url       22866 non-null  object\n",
            " 2   source_domain  22866 non-null  object\n",
            " 3   tweet_num      23196 non-null  int64 \n",
            " 4   real           23196 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 906.2+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R_YW5RBJKpn"
      },
      "source": [
        "We can see that the data consists of about 23K enteries/rows and 4 features/columns with string and integer data types\n",
        "\n",
        "|Coloumn Name|Description|\n",
        "|---|---|\n",
        "|title\t|title of the article\n",
        "|news_url|URL of the news site\n",
        "|source_domain\t|web domain where article was posted\n",
        "|tweet_num\t|number of retweets for this article.\n",
        "|real\t|label column, where 1 is real and 0 is fake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJb2A0MDKumE"
      },
      "source": [
        "What is the average number of re-tweets a news article gets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju8zzLuNtmQ-",
        "outputId": "5d8ec784-252e-4b79-d05c-218bbc0fa1dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    23196.000000\n",
              "mean        88.956803\n",
              "std        488.694592\n",
              "min          0.000000\n",
              "25%         11.000000\n",
              "50%         37.000000\n",
              "75%         65.000000\n",
              "max      29060.000000\n",
              "Name: tweet_num, dtype: float64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['tweet_num'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5GvBEI0LD0b"
      },
      "source": [
        "There is a big difference between max and mean/50% quartile, this could mean some sources are more popular than others.\n",
        "\n",
        "What are the different news sources?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMT0SpiVtO7X",
        "outputId": "98b63271-c65c-4ce9-bd42-3bccb32e20e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "people.com                1786\n",
              "www.dailymail.co.uk        964\n",
              "en.wikipedia.org           741\n",
              "www.usmagazine.com         709\n",
              "www.etonline.com           666\n",
              "                          ... \n",
              "bioguide.congress.gov        1\n",
              "dailyheadlines.net           1\n",
              "www.duggarfamily.com         1\n",
              "www.naturallycurly.com       1\n",
              "flashnewscorner.com          1\n",
              "Name: source_domain, Length: 2441, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['source_domain'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bntO09oYLzJ1"
      },
      "source": [
        "How many unique values does each column in the dataset have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSzmD81btcED",
        "outputId": "a70748dd-1124-4bb6-b296-a13f273a5a58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "title            21724\n",
              "news_url         21658\n",
              "source_domain     2441\n",
              "tweet_num          825\n",
              "real                 2\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLUn-tJwMPW5"
      },
      "source": [
        "An example news article title:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6AfzPTA2vkH4",
        "outputId": "d24d5f2e-ec80-4a40-f98f-ddf1226c0b73"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Kandi Burruss Explodes Over Rape Accusation on 'Real Housewives of Atlanta' Reunion (Video)\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['title'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0zhymHBMco_"
      },
      "source": [
        "How balanced is the dataset?\n",
        "\n",
        "About 1/3 of the dataset is fake news. Since it is fairly balanced and there are only 23K entries to work with, we will not be balacing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjSsh4U2vJv2",
        "outputId": "c31dee33-3769-44b7-cce8-682718fe6e39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    17441\n",
              "0     5755\n",
              "Name: real, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['real'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHrsIOlUNfRa"
      },
      "source": [
        "How many character long are these titles?\n",
        "\n",
        "Since they are just article titles, the max length is not more than 200 characters long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ZGJ9BcgSvmrP",
        "outputId": "e7f8c7df-8bcc-4a2a-88db-6a3ab6951a3d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df[\"title\"].str.len().plot.hist()\n",
        "median_len = df['title'].str.len().median()\n",
        "plt.xlabel('Comment length (in characters)')\n",
        "plt.title(f'Histogram of comment lengths; median = {median_len}');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkHd9FBOR8q8"
      },
      "source": [
        "Does the number of re-tweets an article gets relate to it being fake?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "772ZOmXNwC7b",
        "outputId": "fde2a126-e521-4bb3-f9a8-5136782f8d48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "real\n",
              "0    13.0\n",
              "1    44.0\n",
              "dtype: float64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.groupby('real').apply(lambda d: d['tweet_num'].median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVOBh01cwmK4"
      },
      "source": [
        "We can see that fake artiicles have fewer retweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHE53oaCVV3x"
      },
      "source": [
        "What news websites spread the most fake news?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeCFQTSYTW3L",
        "outputId": "3fe92dc6-7474-4895-9894-5d7b128bf681"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "hollywoodlife.com         460\n",
              "people.com                216\n",
              "www.dailymail.co.uk       194\n",
              "radaronline.com           174\n",
              "www.eonline.com           154\n",
              "                         ... \n",
              "sffq.aviazione.biz          1\n",
              "bieber.trendolizer.com      1\n",
              "usa24info.com               1\n",
              "www.nj.com                  1\n",
              "flashnewscorner.com         1\n",
              "Name: source_domain, Length: 971, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fake_df=df[(df['real']==0)]\n",
        "fake_df['source_domain'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6BC_jAxWRIO"
      },
      "source": [
        "It can be seen that it's usually the celebrity based magazines and news channels that spread the most fake news. Some of these are at the top of the list also because they have more entries in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJRwfNg0oNMU"
      },
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kAUeshtxJLS"
      },
      "outputs": [],
      "source": [
        "target = 'real'\n",
        "predictors = 'title'\n",
        "\n",
        "X = df[predictors].values\n",
        "y = df[target].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYKlgZQpxORM",
        "outputId": "8c2ad848-02e8-4c46-b730-8ce837d60d39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: (23196,)\n",
            "y shape: (23196,)\n"
          ]
        }
      ],
      "source": [
        "print('X shape: {}'.format(X.shape))\n",
        "print('y shape: {}'.format(y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s5dIoDExPhw"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RZZdWvlxcpa",
        "outputId": "99e6bce2-3e0e-423d-c8cd-a15f7ad4bc2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16237,) (16237,)\n",
            "(6959,) (6959,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtO73ZJyY5hB"
      },
      "source": [
        "About 16K training data and 7K test data.\n",
        "\n",
        "What is the baseline accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTgmcrzZxzbI",
        "outputId": "4c165efa-f78e-4409-b8f9-8e57db92242e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([0, 1]), array([ 4065, 12172]))\n",
            "(array([0, 1]), array([1690, 5269]))\n"
          ]
        }
      ],
      "source": [
        "print(np.unique(y_train, return_counts=True))\n",
        "print(np.unique(y_test, return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCw0oUv5xfYv",
        "outputId": "229f1e96-7b7d-4775-e4de-fa834d0eb4d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baseline accuracy: 0.7519\n"
          ]
        }
      ],
      "source": [
        "counts = df['real'].value_counts()/df.shape[0]\n",
        "print('baseline accuracy: {:0.4f}'.format(counts[1]))                      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uf0k2spZL5h"
      },
      "source": [
        "# **Machine Learning**\n",
        "\n",
        "We will be using different text classification techniques to build models that can predict if an article is fake or real. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWrr4LkmoS8q"
      },
      "source": [
        "### **Set Models- Bag of words and it's variations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlaSSCTjqYyk"
      },
      "source": [
        "> Unigram Bag-of-words Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePGjAZqD2z4l"
      },
      "source": [
        "Our initial approach uses the bag of words model to see if we can perform better than 75% our baseline.\n",
        "\n",
        "Our first step, would be to convert the text into vectors to feed it into the model. We tried various values of max_tokens, as big as 20K. However since the data is limited, 5K tokens were enough to give good accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JE_tiT6gj5x"
      },
      "outputs": [],
      "source": [
        "max_tokens = 5000\n",
        "text_vectorization_1 = TextVectorization(max_tokens=max_tokens, output_mode=\"multi_hot\")\n",
        "text_vectorization_1.adapt(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5XD4b0uglYC"
      },
      "outputs": [],
      "source": [
        "X_train_1= text_vectorization_1(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpO1CwO0gm1o"
      },
      "outputs": [],
      "source": [
        "X_test_1= text_vectorization_1(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2ntuvKtu2Sp"
      },
      "outputs": [],
      "source": [
        "patience = 5 \n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT3cLJ8ngp10"
      },
      "outputs": [],
      "source": [
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zQKz4I_grcM"
      },
      "outputs": [],
      "source": [
        "input_size = X_train_1.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXflJQm-5WFE"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu', input_shape=(input_size,),kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2Wqk26p3z_i",
        "outputId": "e759cf80-e6a9-4df1-b34b-7adbb2a2d49e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "89/89 [==============================] - 3s 22ms/step - loss: 0.6779 - accuracy: 0.7588 - val_loss: 0.5428 - val_accuracy: 0.8188\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 0.5015 - accuracy: 0.8335 - val_loss: 0.4880 - val_accuracy: 0.8266\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 0.4491 - accuracy: 0.8506 - val_loss: 0.4677 - val_accuracy: 0.8309\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 2s 23ms/step - loss: 0.4237 - accuracy: 0.8589 - val_loss: 0.4599 - val_accuracy: 0.8335\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 3s 28ms/step - loss: 0.4046 - accuracy: 0.8656 - val_loss: 0.4544 - val_accuracy: 0.8348\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 2s 27ms/step - loss: 0.3938 - accuracy: 0.8700 - val_loss: 0.4491 - val_accuracy: 0.8329\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 0.3837 - accuracy: 0.8743 - val_loss: 0.4490 - val_accuracy: 0.8381\n",
            "Epoch 8/30\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 0.3658 - accuracy: 0.8816 - val_loss: 0.4442 - val_accuracy: 0.8358\n",
            "Epoch 9/30\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.3578 - accuracy: 0.8839 - val_loss: 0.4451 - val_accuracy: 0.8358\n",
            "Epoch 10/30\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.3528 - accuracy: 0.8859 - val_loss: 0.4434 - val_accuracy: 0.8362\n",
            "Epoch 11/30\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.3413 - accuracy: 0.8929 - val_loss: 0.4567 - val_accuracy: 0.8346\n",
            "Epoch 12/30\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.3321 - accuracy: 0.8944 - val_loss: 0.4595 - val_accuracy: 0.8311\n",
            "Epoch 13/30\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.3259 - accuracy: 0.8969 - val_loss: 0.4595 - val_accuracy: 0.8321\n",
            "Epoch 14/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.3185 - accuracy: 0.9015 - val_loss: 0.4588 - val_accuracy: 0.8278\n",
            "Epoch 15/30\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.3152 - accuracy: 0.9041 - val_loss: 0.4696 - val_accuracy: 0.8257\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_1, y_train, epochs=30, batch_size=128, validation_split=0.3, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6gHrrmw_NtQ",
        "outputId": "04d31d75-3c2d-43c3-ce76-93113c93b923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "218/218 [==============================] - 1s 3ms/step - loss: 0.4670 - accuracy: 0.8273\n",
            "test accuracy:  0.827\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test_1,y_test)\n",
        "print(f\"test accuracy: {test_acc: .3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx3_UJRI5LKW"
      },
      "source": [
        "After experimenting with various parameters here's what we found:\n",
        "- Batch Normalizing the data was infact reducing the accuracy by a lot.\n",
        "- Dropout of 0.5 worked best, 0.2 and 0.7 were affecting the accuracy negatively\n",
        "- Regularizing as L2 normalization worked better than L1\n",
        "- Multiple dense layers and layer size of 100 worked better than 32,64,128 etc\n",
        "\n",
        "Other parameters were also tuned like activation functions, optimizer and batch_size to achive this model with an overall accuracy that seemed to peak around 83%. Although it does perform better than baseline, accuracy seems to have plateaued. \n",
        "\n",
        "How about n-gram bag-of-word_models? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU9s_F2fQ_CY"
      },
      "source": [
        "> N-grams Bag-of-words model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmo5_UkQpjxH"
      },
      "source": [
        "This model uses the same structure as the one used in bag of words. However,with this model we are using up to 3 words per sequence to see if there can be any improvement.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2_yVwzWSti_"
      },
      "outputs": [],
      "source": [
        "text_vectorization_1 = TextVectorization(max_tokens = max_tokens, output_mode = \"multi_hot\", ngrams = 3)\n",
        "text_vectorization_1.adapt(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONdqYjNPStjL"
      },
      "outputs": [],
      "source": [
        "X_train_1 = text_vectorization_1(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjRkexHKStjM"
      },
      "outputs": [],
      "source": [
        "X_test_1 = text_vectorization_1(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH2BvOHS8WZg"
      },
      "outputs": [],
      "source": [
        "input_size = X_train_1.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKrlvTmzStjM"
      },
      "outputs": [],
      "source": [
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqQET8-qTO8e"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu', input_shape=(input_size,),kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6husLZ5GTTB4",
        "outputId": "af64d666-f376-4f9a-c559-224dd706cacc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "89/89 [==============================] - 3s 17ms/step - loss: 0.7583 - accuracy: 0.7501 - val_loss: 0.6107 - val_accuracy: 0.8095\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.5656 - accuracy: 0.8187 - val_loss: 0.5303 - val_accuracy: 0.8296\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.4887 - accuracy: 0.8507 - val_loss: 0.4925 - val_accuracy: 0.8303\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 0.4511 - accuracy: 0.8558 - val_loss: 0.4750 - val_accuracy: 0.8315\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 2s 22ms/step - loss: 0.4231 - accuracy: 0.8633 - val_loss: 0.4771 - val_accuracy: 0.8331\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 0.4051 - accuracy: 0.8692 - val_loss: 0.4580 - val_accuracy: 0.8344\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.3912 - accuracy: 0.8727 - val_loss: 0.4610 - val_accuracy: 0.8333\n",
            "Epoch 8/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.3757 - accuracy: 0.8794 - val_loss: 0.4559 - val_accuracy: 0.8313\n",
            "Epoch 9/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.3635 - accuracy: 0.8839 - val_loss: 0.4627 - val_accuracy: 0.8348\n",
            "Epoch 10/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.3555 - accuracy: 0.8875 - val_loss: 0.4654 - val_accuracy: 0.8284\n",
            "Epoch 11/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.3498 - accuracy: 0.8870 - val_loss: 0.4632 - val_accuracy: 0.8331\n",
            "Epoch 12/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.3409 - accuracy: 0.8918 - val_loss: 0.4556 - val_accuracy: 0.8311\n",
            "Epoch 13/30\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 0.3351 - accuracy: 0.8984 - val_loss: 0.4602 - val_accuracy: 0.8296\n",
            "Epoch 14/30\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 0.3359 - accuracy: 0.8918 - val_loss: 0.4798 - val_accuracy: 0.8280\n",
            "Epoch 15/30\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 0.3370 - accuracy: 0.8936 - val_loss: 0.4623 - val_accuracy: 0.8315\n",
            "Epoch 16/30\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 0.3326 - accuracy: 0.8952 - val_loss: 0.4633 - val_accuracy: 0.8253\n",
            "Epoch 17/30\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 0.3341 - accuracy: 0.8971 - val_loss: 0.4652 - val_accuracy: 0.8292\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_1, y_train, epochs=30, batch_size=128, validation_split=0.3, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNngQUOvTTB5",
        "outputId": "dc491c19-987e-4643-9992-06111fd990bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "218/218 [==============================] - 1s 4ms/step - loss: 0.4765 - accuracy: 0.8245\n",
            "test accuracy:  0.825\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test_1,y_test)\n",
        "print(f\"test accuracy: {test_acc: .3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOadqHRw6G9r"
      },
      "source": [
        "Using a 3-gram model ends up giving around the same accuracy ~83%.  \n",
        "\n",
        "How about using TF-IDF? Would adding a bit more information to this representation by counting how many times n-gram occurs, improve the accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mub7coGKp41Z"
      },
      "source": [
        "> Bag-of-words using TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4rCcqkCSTY5"
      },
      "source": [
        "The same model as our previous example, this time using TF-IDF. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JudwgO_wprOw"
      },
      "outputs": [],
      "source": [
        "text_vectorization_1 = TextVectorization(max_tokens = 5000, output_mode = \"tf_idf\", ngrams = 3)\n",
        "text_vectorization_1.adapt(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_Pxmi69prOw"
      },
      "outputs": [],
      "source": [
        "X_train_1 = text_vectorization_1(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phjACOaLprOw"
      },
      "outputs": [],
      "source": [
        "X_test_1 = text_vectorization_1(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBMW3VXYprOx"
      },
      "outputs": [],
      "source": [
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6arnhlwYprOx"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu', input_shape=(X_train_1.shape[1],)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jxpf_57prOx",
        "outputId": "ebaedbb4-c934-42e3-b5f4-18c9c6b75df7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "89/89 [==============================] - 3s 17ms/step - loss: 0.8161 - accuracy: 0.5908 - val_loss: 0.5606 - val_accuracy: 0.7453\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.6599 - accuracy: 0.6907 - val_loss: 0.5621 - val_accuracy: 0.7453\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.5807 - accuracy: 0.7372 - val_loss: 0.4731 - val_accuracy: 0.7818\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.4900 - accuracy: 0.7808 - val_loss: 0.4319 - val_accuracy: 0.8130\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.4287 - accuracy: 0.8168 - val_loss: 0.4715 - val_accuracy: 0.8048\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.3885 - accuracy: 0.8360 - val_loss: 0.4032 - val_accuracy: 0.8307\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.3586 - accuracy: 0.8523 - val_loss: 0.4080 - val_accuracy: 0.8372\n",
            "Epoch 8/30\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.3361 - accuracy: 0.8628 - val_loss: 0.4803 - val_accuracy: 0.8313\n",
            "Epoch 9/30\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.3132 - accuracy: 0.8761 - val_loss: 0.4414 - val_accuracy: 0.8335\n",
            "Epoch 10/30\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.2916 - accuracy: 0.8839 - val_loss: 0.4384 - val_accuracy: 0.8325\n",
            "Epoch 11/30\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.2698 - accuracy: 0.8961 - val_loss: 0.4438 - val_accuracy: 0.8360\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_1, y_train, epochs=30, batch_size=128, validation_split=0.3, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0smTKndXprOx",
        "outputId": "a045e725-bdc9-420b-c75f-8a52fb06d145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "218/218 [==============================] - 1s 3ms/step - loss: 0.4453 - accuracy: 0.8327\n",
            "test accuracy:  0.833\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test_1,y_test)\n",
        "print(f\"test accuracy: {test_acc: .3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSpEMXtvbiD9"
      },
      "source": [
        "Bag-of-Word Model Conclusions: \n",
        "\n",
        "All these models once fine-tuned end up performing at around 83% test accuracy. \n",
        "\n",
        "Would it be helpful if we take word-order into consideration? From here, we are gonna change our approach and use sequence models to see if we can go beyond 83% accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN8GWW-voUPf"
      },
      "source": [
        "### **Sequence models- Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZD6lXuOq6VB"
      },
      "source": [
        "> Embedding layer trained from scratch\n",
        "\n",
        "We would be representing our input samples as sequences, vectorizing them and feeding these inputs to models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBJZweadTgyl"
      },
      "outputs": [],
      "source": [
        "max_length = 200\n",
        "max_tokens = 5000 \n",
        "text_vectorization_3 = layers.TextVectorization(max_tokens=max_tokens,output_mode=\"int\",output_sequence_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmA1qpjYToS6"
      },
      "outputs": [],
      "source": [
        "text_vectorization_3.adapt(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIGB9lLyTrWP"
      },
      "outputs": [],
      "source": [
        "X_train_3= text_vectorization_3(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL1rjr_pTtBc"
      },
      "outputs": [],
      "source": [
        "X_test_3= text_vectorization_3(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nobw2SuGTusL"
      },
      "outputs": [],
      "source": [
        "patience=5 \n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=patience)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmB4PJG5TxG1"
      },
      "outputs": [],
      "source": [
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akhQwpitT3mB",
        "outputId": "f3b16aca-7646-4291-fc52-3d8d21a45ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 32)          160000    \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 64)          2112      \n",
            "                                                                 \n",
            " gru (GRU)                   (None, None, 32)          9408      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 32)                6336      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                2112      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 180,033\n",
            "Trainable params: 180,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=32,mask_zero=True)(inputs)\n",
        "x=layers.Dense(64, activation=\"relu\",kernel_regularizer=keras.regularizers.l1(0.001))(embedded)\n",
        "x = layers.GRU(32,return_sequences=True)(x)\n",
        "x = layers.GRU(32)(x)\n",
        "x=layers.Dense(64, activation=\"relu\",kernel_regularizer=keras.regularizers.l1(0.001))(x) \n",
        "                                                                         \n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puAhGMrZT5q9",
        "outputId": "30a1d827-af3f-4f16-f3b8-588abdb9fe78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "178/178 [==============================] - 62s 301ms/step - loss: 0.7770 - accuracy: 0.7719 - val_loss: 0.5568 - val_accuracy: 0.8222\n",
            "Epoch 2/100\n",
            "178/178 [==============================] - 49s 276ms/step - loss: 0.4677 - accuracy: 0.8480 - val_loss: 0.4651 - val_accuracy: 0.8321\n",
            "Epoch 3/100\n",
            "178/178 [==============================] - 47s 266ms/step - loss: 0.3960 - accuracy: 0.8689 - val_loss: 0.4470 - val_accuracy: 0.8327\n",
            "Epoch 4/100\n",
            "178/178 [==============================] - 54s 302ms/step - loss: 0.3622 - accuracy: 0.8786 - val_loss: 0.4396 - val_accuracy: 0.8325\n",
            "Epoch 5/100\n",
            "178/178 [==============================] - 49s 276ms/step - loss: 0.3395 - accuracy: 0.8834 - val_loss: 0.4449 - val_accuracy: 0.8274\n",
            "Epoch 6/100\n",
            "178/178 [==============================] - 52s 291ms/step - loss: 0.3254 - accuracy: 0.8889 - val_loss: 0.4595 - val_accuracy: 0.8319\n",
            "Epoch 7/100\n",
            "178/178 [==============================] - 51s 285ms/step - loss: 0.3165 - accuracy: 0.8890 - val_loss: 0.4506 - val_accuracy: 0.8056\n",
            "Epoch 8/100\n",
            "178/178 [==============================] - 50s 280ms/step - loss: 0.3078 - accuracy: 0.8920 - val_loss: 0.4815 - val_accuracy: 0.8352\n",
            "Epoch 9/100\n",
            "178/178 [==============================] - 50s 281ms/step - loss: 0.3005 - accuracy: 0.8963 - val_loss: 0.4655 - val_accuracy: 0.8253\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f446a526070>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train_3,y_train, epochs=100,batch_size=64,callbacks=[early_stopping],validation_split=0.3, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsT9zISMT7oz",
        "outputId": "8e61485c-116b-4005-d82b-d489edd6f991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "218/218 [==============================] - 11s 49ms/step - loss: 0.4679 - accuracy: 0.8248\n",
            "test accuracy:  0.825\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test_3, y_test)\n",
        "print(f\"test accuracy: {test_acc: .3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0AfGrqepx5E"
      },
      "source": [
        "For this manually defined word embedding model, max_length of 200 worked the best, this could be because it is the average number of characters in the title of the dataset. Apart from this, few other observations made were:\n",
        "- Stacked GRU performed better than any other neural network\n",
        "- Dense layers at the start of the model (before GRU) helped improve the model\n",
        "- These combinations of 32 and 64 layer sizes increased the accuracy\n",
        "- L1 normalization drastically improved the accuracy \n",
        "- A lower batch size of 64 worked better unlike for the bag-of-words model\n",
        "-Padding and masking in the embedding layer also improved the model by some extent.\n",
        "\n",
        "Other parameters like Batch Normalization, Bidirectional layers, adding more layers negatively impacted the model.\n",
        "\n",
        "Even word embedding models could not go beyond th 83% accuracy mark. Since there is not much data to train these embeddings on, we could use already learned embeddings to see if it could work better with our dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLZbHb6jq-Oq"
      },
      "source": [
        "> Predefined Embedding Model- GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmnHwRxMrBR1",
        "outputId": "e511f7ee-c48b-41c6-9443-536598c9fddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-13 04:11:15--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-12-13 04:11:15--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-12-13 04:11:16--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.07MB/s    in 2m 39s  \n",
            "\n",
            "2022-12-13 04:13:56 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAGJGZ6rrEdA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SIonvwprGY_",
        "outputId": "67bdbdbd-5927-4033-978f-5ad189906b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "embeddings_index = {} \n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "  \n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_burAGZrJET"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 \n",
        "  \n",
        "vocabulary = text_vectorization_3.get_vocabulary()            \n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))   \n",
        " \n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))     \n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:                         \n",
        "        embedding_matrix[i] = embedding_vector   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIVd1a2mrLNh"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8SktuYmrQsD"
      },
      "outputs": [],
      "source": [
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTpc39DlrOfY",
        "outputId": "811e912a-803b-4d52-821c-85093f7f02b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 100)         500000    \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 64)          6464      \n",
            "                                                                 \n",
            " gru (GRU)                   (None, None, 32)          9408      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 32)                6336      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                2112      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 524,385\n",
            "Trainable params: 24,385\n",
            "Non-trainable params: 500,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x=layers.Dense(64, activation=\"relu\",kernel_regularizer=keras.regularizers.l1(0.001))(embedded)\n",
        "x = layers.GRU(32,return_sequences=True)(x)\n",
        "x = layers.GRU(32)(x)\n",
        "x=layers.Dense(64, activation=\"relu\",kernel_regularizer=keras.regularizers.l1(0.001))(x) \n",
        "                                                                         \n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGbP-QE7rVBU",
        "outputId": "6084fbb2-c260-478c-ff65-fbe780fc0c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "178/178 [==============================] - 66s 317ms/step - loss: 1.0101 - accuracy: 0.7586 - val_loss: 0.7720 - val_accuracy: 0.7785\n",
            "Epoch 2/100\n",
            "178/178 [==============================] - 55s 307ms/step - loss: 0.6765 - accuracy: 0.7868 - val_loss: 0.6198 - val_accuracy: 0.8003\n",
            "Epoch 3/100\n",
            "178/178 [==============================] - 48s 272ms/step - loss: 0.5984 - accuracy: 0.7955 - val_loss: 0.5939 - val_accuracy: 0.7863\n",
            "Epoch 4/100\n",
            "178/178 [==============================] - 52s 291ms/step - loss: 0.5654 - accuracy: 0.8039 - val_loss: 0.5473 - val_accuracy: 0.8021\n",
            "Epoch 5/100\n",
            "178/178 [==============================] - 50s 282ms/step - loss: 0.5383 - accuracy: 0.8125 - val_loss: 0.5218 - val_accuracy: 0.8183\n",
            "Epoch 6/100\n",
            "178/178 [==============================] - 50s 282ms/step - loss: 0.5233 - accuracy: 0.8128 - val_loss: 0.5069 - val_accuracy: 0.8140\n",
            "Epoch 7/100\n",
            "178/178 [==============================] - 49s 277ms/step - loss: 0.5100 - accuracy: 0.8154 - val_loss: 0.5057 - val_accuracy: 0.8192\n",
            "Epoch 8/100\n",
            "178/178 [==============================] - 50s 280ms/step - loss: 0.4994 - accuracy: 0.8224 - val_loss: 0.5596 - val_accuracy: 0.8003\n",
            "Epoch 9/100\n",
            "178/178 [==============================] - 49s 273ms/step - loss: 0.4892 - accuracy: 0.8238 - val_loss: 0.4873 - val_accuracy: 0.8237\n",
            "Epoch 10/100\n",
            "178/178 [==============================] - 47s 262ms/step - loss: 0.4833 - accuracy: 0.8265 - val_loss: 0.4799 - val_accuracy: 0.8235\n",
            "Epoch 11/100\n",
            "178/178 [==============================] - 50s 284ms/step - loss: 0.4730 - accuracy: 0.8296 - val_loss: 0.4820 - val_accuracy: 0.8237\n",
            "Epoch 12/100\n",
            "178/178 [==============================] - 53s 298ms/step - loss: 0.4711 - accuracy: 0.8310 - val_loss: 0.5093 - val_accuracy: 0.8132\n",
            "Epoch 13/100\n",
            "178/178 [==============================] - 48s 269ms/step - loss: 0.4676 - accuracy: 0.8326 - val_loss: 0.4796 - val_accuracy: 0.8186\n",
            "Epoch 14/100\n",
            "178/178 [==============================] - 48s 270ms/step - loss: 0.4611 - accuracy: 0.8333 - val_loss: 0.4712 - val_accuracy: 0.8303\n",
            "Epoch 15/100\n",
            "178/178 [==============================] - 47s 262ms/step - loss: 0.4563 - accuracy: 0.8343 - val_loss: 0.4865 - val_accuracy: 0.8243\n",
            "Epoch 16/100\n",
            "178/178 [==============================] - 47s 264ms/step - loss: 0.4537 - accuracy: 0.8351 - val_loss: 0.4700 - val_accuracy: 0.8280\n",
            "Epoch 17/100\n",
            "178/178 [==============================] - 46s 261ms/step - loss: 0.4524 - accuracy: 0.8374 - val_loss: 0.4772 - val_accuracy: 0.8290\n",
            "Epoch 18/100\n",
            "178/178 [==============================] - 48s 270ms/step - loss: 0.4511 - accuracy: 0.8365 - val_loss: 0.4959 - val_accuracy: 0.8009\n",
            "Epoch 19/100\n",
            "178/178 [==============================] - 49s 274ms/step - loss: 0.4414 - accuracy: 0.8425 - val_loss: 0.4695 - val_accuracy: 0.8220\n",
            "Epoch 20/100\n",
            "178/178 [==============================] - 49s 276ms/step - loss: 0.4417 - accuracy: 0.8397 - val_loss: 0.5574 - val_accuracy: 0.8030\n",
            "Epoch 21/100\n",
            "178/178 [==============================] - 53s 299ms/step - loss: 0.4382 - accuracy: 0.8422 - val_loss: 0.5031 - val_accuracy: 0.8153\n",
            "Epoch 22/100\n",
            "178/178 [==============================] - 49s 275ms/step - loss: 0.4346 - accuracy: 0.8465 - val_loss: 0.4776 - val_accuracy: 0.8229\n",
            "Epoch 23/100\n",
            "178/178 [==============================] - 48s 272ms/step - loss: 0.4328 - accuracy: 0.8445 - val_loss: 0.5794 - val_accuracy: 0.8128\n",
            "Epoch 24/100\n",
            "178/178 [==============================] - 48s 272ms/step - loss: 0.4272 - accuracy: 0.8480 - val_loss: 0.4695 - val_accuracy: 0.8239\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f44633cfd90>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train_3,y_train, epochs=100,batch_size=64,callbacks=[early_stopping],validation_split=0.3, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecGitNi7rXM9",
        "outputId": "29084a24-6fce-45b1-ec2b-006d917af204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "218/218 [==============================] - 10s 47ms/step - loss: 0.4813 - accuracy: 0.8225\n",
            "test accuracy:  0.823\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test_3, y_test)\n",
        "print(f\"test accuracy: {test_acc: .3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnKlO-Eebzpy"
      },
      "source": [
        "We used the GloVe embeddings for this dataset since it has over 400K work vectors and one of the most popular ones. Identical to the previous model this model had 100 dimensional pretrained embeddings instead of learned embeddings. \n",
        "\n",
        "However it did not work as expected, this could be because either the data was suffienct for the previous embedding model to learn or because these predefined embeddings do not work for this specific dataset.\n",
        "\n",
        "Could the concept of \"attention\" help recognize more important features? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ1MqiBroWF6"
      },
      "source": [
        "### **The Transformer Architecture**\n",
        "\n",
        "We would be using a transformer encoder to make text classifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFezZRbM1Gok"
      },
      "source": [
        "> Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtYw_NrId6x2"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim                         \n",
        "        self.dense_dim = dense_dim                         \n",
        "        self.num_heads = num_heads                         \n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim,activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "    def call(self, inputs, mask=None):                    \n",
        "        if mask is not None:                              \n",
        "            mask = mask[:, tf.newaxis, :]                 \n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "  \n",
        "    def get_config(self):                                 \n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKTBeS-J5fVL"
      },
      "outputs": [],
      "source": [
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUf_4j9A_ex5",
        "outputId": "11dd8706-e32d-438a-c0e5-e1b5bae74030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         1280000   \n",
            "                                                                 \n",
            " transformer_encoder (Transf  (None, None, 256)        806688    \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 256)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,086,945\n",
            "Trainable params: 2,086,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 5000\n",
        "embed_dim = 256\n",
        "num_heads = 3\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ9W3qiq_ikb",
        "outputId": "341192f0-c32e-478f-9e68-63bf96f9e51a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "178/178 [==============================] - 10s 48ms/step - loss: 0.5737 - accuracy: 0.7516 - val_loss: 0.4338 - val_accuracy: 0.8266\n",
            "Epoch 2/100\n",
            "178/178 [==============================] - 8s 46ms/step - loss: 0.3829 - accuracy: 0.8390 - val_loss: 0.3988 - val_accuracy: 0.8317\n",
            "Epoch 3/100\n",
            "178/178 [==============================] - 8s 46ms/step - loss: 0.3335 - accuracy: 0.8604 - val_loss: 0.3840 - val_accuracy: 0.8362\n",
            "Epoch 4/100\n",
            "178/178 [==============================] - 8s 47ms/step - loss: 0.2993 - accuracy: 0.8778 - val_loss: 0.4198 - val_accuracy: 0.8362\n",
            "Epoch 5/100\n",
            "178/178 [==============================] - 8s 47ms/step - loss: 0.2810 - accuracy: 0.8851 - val_loss: 0.4173 - val_accuracy: 0.8325\n",
            "Epoch 6/100\n",
            "178/178 [==============================] - 8s 47ms/step - loss: 0.2666 - accuracy: 0.8934 - val_loss: 0.4278 - val_accuracy: 0.8325\n",
            "Epoch 7/100\n",
            "178/178 [==============================] - 8s 47ms/step - loss: 0.2542 - accuracy: 0.8934 - val_loss: 0.4106 - val_accuracy: 0.8344\n",
            "Epoch 8/100\n",
            "178/178 [==============================] - 8s 47ms/step - loss: 0.2435 - accuracy: 0.9009 - val_loss: 0.4201 - val_accuracy: 0.8335\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb269c58610>"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train_3,y_train, epochs=100,batch_size=64,callbacks=[early_stopping],validation_split=0.3, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBRNjVQv5yC4",
        "outputId": "7c34bcd2-a1c3-4b0a-946d-75502f46efb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "218/218 [==============================] - 2s 9ms/step - loss: 0.4238 - accuracy: 0.8339\n",
            "test accuracy:  0.834\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test_3, y_test)\n",
        "print(f\"test accuracy: {test_acc: .3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgD9_bva1uy0"
      },
      "source": [
        "This model applies the concepts of multi-headed attention, with 3 inputs, combinations of dense layers and layer normalization. We just need the encoder part of the architecture to solve text classification problems.\n",
        "\n",
        "Again, about the same accuracy. Would giving a word order using positional embedding help?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exfIOaVn1TK8"
      },
      "source": [
        "> Transformer Encoder with Positional Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdfwLZ4H_mlK"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70ImJjfu_rUl",
        "outputId": "2680becc-28ac-4ae6-a0c7-532297d57fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posit  (None, None, 256)        1433600   \n",
            " ionalEmbedding)                                                 \n",
            "                                                                 \n",
            " transformer_encoder (Transf  (None, None, 256)        560192    \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 256)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,994,049\n",
            "Trainable params: 1,994,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "178/178 [==============================] - 12s 37ms/step - loss: 0.5934 - accuracy: 0.7544 - val_loss: 0.4023 - val_accuracy: 0.8259\n",
            "Epoch 2/100\n",
            "178/178 [==============================] - 6s 35ms/step - loss: 0.3574 - accuracy: 0.8521 - val_loss: 0.3844 - val_accuracy: 0.8352\n",
            "Epoch 3/100\n",
            "178/178 [==============================] - 6s 36ms/step - loss: 0.3007 - accuracy: 0.8780 - val_loss: 0.4087 - val_accuracy: 0.8296\n",
            "Epoch 4/100\n",
            "178/178 [==============================] - 6s 36ms/step - loss: 0.2616 - accuracy: 0.8961 - val_loss: 0.5172 - val_accuracy: 0.8023\n",
            "Epoch 5/100\n",
            "178/178 [==============================] - 6s 36ms/step - loss: 0.2355 - accuracy: 0.9033 - val_loss: 0.4646 - val_accuracy: 0.8286\n",
            "Epoch 6/100\n",
            "178/178 [==============================] - 6s 36ms/step - loss: 0.2114 - accuracy: 0.9144 - val_loss: 0.5076 - val_accuracy: 0.8169\n",
            "Epoch 7/100\n",
            "178/178 [==============================] - 7s 37ms/step - loss: 0.1923 - accuracy: 0.9241 - val_loss: 0.5404 - val_accuracy: 0.8081\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f67dc3896d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "vocab_size = 5000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 64\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "model.fit(X_train_3,y_train, epochs=100,batch_size=64,callbacks=[early_stopping],validation_split=0.3, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnV2AqmE_84B",
        "outputId": "a63f85c8-a171-480c-e345-778da1eee54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "218/218 [==============================] - 2s 8ms/step - loss: 0.5347 - accuracy: 0.8086\n",
            "test accuracy:  0.809\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test_3, y_test)\n",
        "print(f\"test accuracy: {test_acc: .3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdG7eoMZ3ujW"
      },
      "source": [
        "Adding order to the model does not help the model. Let's try tuning it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gctbOJzM0AW"
      },
      "source": [
        "> Tuned Transformer Model using Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ym9Exzd6cD1"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "  vocab_size = 5000\n",
        "  sequence_length = 600\n",
        "  embed_dim = 256\n",
        "  num_heads = 2\n",
        "  dense_dim = 64\n",
        "\n",
        "  inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "  x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "  x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "  x = layers.GlobalMaxPooling1D()(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHiez8Tz-DkO"
      },
      "outputs": [],
      "source": [
        "default_params = {\n",
        "    'batch_size': 128,\n",
        "    'embed_dim': 256,\n",
        "    'num_heads': 3,\n",
        "    'dense_dim' : 32\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ_o91k273Sm"
      },
      "outputs": [],
      "source": [
        "def evaluate_params(params, verbose=1):\n",
        "    \"\"\" Return validation accuracy for a model built and trained using the given parameters.\n",
        "    args:\n",
        "        params: a Python dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    # default parameters are used if not supplied\n",
        "    pars = default_params.copy()\n",
        "    pars.update(params)\n",
        "\n",
        "    batch_size = pars['batch_size']\n",
        "    embed_dim = pars['embed_dim'] \n",
        "    num_head =  pars[\"num_heads\"],\n",
        "    dense_dim = pars['dense_dim']\n",
        " \n",
        "\n",
        "    model = get_model()\n",
        "    model.compile(optimizer='rmsprop',\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "    \n",
        "    history = model.fit(X_train_3, y_train, epochs=30, callbacks=[early_stopping], batch_size=batch_size, \n",
        "                        validation_split=0.3, verbose=verbose)\n",
        "    mean_acc = np.mean(history.history['val_accuracy'][-2:])   \n",
        "\n",
        "    return pars, mean_acc, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx_paF64-6lg"
      },
      "outputs": [],
      "source": [
        "def grid_search(param_grid, verbose=1):\n",
        "\n",
        "    params_list = []\n",
        "    acc_list = []\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        print(f\"params: {params}\")\n",
        "        pars, acc, history = evaluate_params(params, verbose=verbose)\n",
        "\n",
        "        params_list.append(pars)\n",
        "        acc_list.append(acc)\n",
        "\n",
        "    return params_list, acc_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSpS2pjzM5nw",
        "outputId": "fa692b3c-6633-42e3-a2d7-fbad1e1d168b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params: {'batch_size': 16, 'dense_dim': 16, 'embed_dim': 256, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5069 - accuracy: 0.7846 - val_loss: 0.4867 - val_accuracy: 0.7566\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3721 - accuracy: 0.8496 - val_loss: 0.4033 - val_accuracy: 0.8292\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3346 - accuracy: 0.8688 - val_loss: 0.4370 - val_accuracy: 0.8132\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3089 - accuracy: 0.8783 - val_loss: 0.4282 - val_accuracy: 0.8290\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2926 - accuracy: 0.8883 - val_loss: 0.4261 - val_accuracy: 0.8198\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2841 - accuracy: 0.8914 - val_loss: 0.4285 - val_accuracy: 0.8237\n",
            "Epoch 7/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2727 - accuracy: 0.8993 - val_loss: 0.4377 - val_accuracy: 0.8251\n",
            "params: {'batch_size': 16, 'dense_dim': 16, 'embed_dim': 256, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5069 - accuracy: 0.7867 - val_loss: 0.4002 - val_accuracy: 0.8350\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3708 - accuracy: 0.8526 - val_loss: 0.4126 - val_accuracy: 0.8296\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3313 - accuracy: 0.8676 - val_loss: 0.4377 - val_accuracy: 0.8352\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3115 - accuracy: 0.8788 - val_loss: 0.4137 - val_accuracy: 0.8173\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2934 - accuracy: 0.8875 - val_loss: 0.4505 - val_accuracy: 0.8261\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2802 - accuracy: 0.8920 - val_loss: 0.4095 - val_accuracy: 0.8284\n",
            "params: {'batch_size': 16, 'dense_dim': 16, 'embed_dim': 256, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5048 - accuracy: 0.7901 - val_loss: 0.4244 - val_accuracy: 0.8144\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3683 - accuracy: 0.8524 - val_loss: 0.3966 - val_accuracy: 0.8323\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3292 - accuracy: 0.8713 - val_loss: 0.4033 - val_accuracy: 0.8317\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 10s 13ms/step - loss: 0.3083 - accuracy: 0.8794 - val_loss: 0.4289 - val_accuracy: 0.8333\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 10s 13ms/step - loss: 0.2959 - accuracy: 0.8861 - val_loss: 0.4244 - val_accuracy: 0.8315\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2833 - accuracy: 0.8917 - val_loss: 0.4311 - val_accuracy: 0.8323\n",
            "Epoch 7/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2781 - accuracy: 0.8967 - val_loss: 0.4288 - val_accuracy: 0.8292\n",
            "params: {'batch_size': 16, 'dense_dim': 16, 'embed_dim': 128, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 10s 13ms/step - loss: 0.4991 - accuracy: 0.7930 - val_loss: 0.3912 - val_accuracy: 0.8333\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3673 - accuracy: 0.8520 - val_loss: 0.3995 - val_accuracy: 0.8366\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3288 - accuracy: 0.8702 - val_loss: 0.4217 - val_accuracy: 0.8220\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3115 - accuracy: 0.8805 - val_loss: 0.4187 - val_accuracy: 0.8171\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2922 - accuracy: 0.8889 - val_loss: 0.4212 - val_accuracy: 0.8274\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2786 - accuracy: 0.8935 - val_loss: 0.4501 - val_accuracy: 0.8179\n",
            "params: {'batch_size': 16, 'dense_dim': 16, 'embed_dim': 128, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.4961 - accuracy: 0.7894 - val_loss: 0.4068 - val_accuracy: 0.8251\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3699 - accuracy: 0.8540 - val_loss: 0.3983 - val_accuracy: 0.8307\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3324 - accuracy: 0.8673 - val_loss: 0.3904 - val_accuracy: 0.8368\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3098 - accuracy: 0.8814 - val_loss: 0.4281 - val_accuracy: 0.8237\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2958 - accuracy: 0.8870 - val_loss: 0.4363 - val_accuracy: 0.8155\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2843 - accuracy: 0.8936 - val_loss: 0.5131 - val_accuracy: 0.7962\n",
            "Epoch 7/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2767 - accuracy: 0.8958 - val_loss: 0.4237 - val_accuracy: 0.8276\n",
            "Epoch 8/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2697 - accuracy: 0.8984 - val_loss: 0.4903 - val_accuracy: 0.8239\n",
            "params: {'batch_size': 16, 'dense_dim': 16, 'embed_dim': 128, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5054 - accuracy: 0.7885 - val_loss: 0.4224 - val_accuracy: 0.8243\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3718 - accuracy: 0.8476 - val_loss: 0.3966 - val_accuracy: 0.8333\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3341 - accuracy: 0.8672 - val_loss: 0.4316 - val_accuracy: 0.8315\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3100 - accuracy: 0.8816 - val_loss: 0.4410 - val_accuracy: 0.8350\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2980 - accuracy: 0.8851 - val_loss: 0.4540 - val_accuracy: 0.8186\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2838 - accuracy: 0.8947 - val_loss: 0.4389 - val_accuracy: 0.8216\n",
            "Epoch 7/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2754 - accuracy: 0.8978 - val_loss: 0.4765 - val_accuracy: 0.8169\n",
            "params: {'batch_size': 16, 'dense_dim': 32, 'embed_dim': 256, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5059 - accuracy: 0.7853 - val_loss: 0.4198 - val_accuracy: 0.8325\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3741 - accuracy: 0.8456 - val_loss: 0.3965 - val_accuracy: 0.8309\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3332 - accuracy: 0.8671 - val_loss: 0.4099 - val_accuracy: 0.8350\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3122 - accuracy: 0.8765 - val_loss: 0.4667 - val_accuracy: 0.8383\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2996 - accuracy: 0.8849 - val_loss: 0.4134 - val_accuracy: 0.8333\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2867 - accuracy: 0.8890 - val_loss: 0.4087 - val_accuracy: 0.8294\n",
            "Epoch 7/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2774 - accuracy: 0.8970 - val_loss: 0.4190 - val_accuracy: 0.8303\n",
            "params: {'batch_size': 16, 'dense_dim': 32, 'embed_dim': 256, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5067 - accuracy: 0.7868 - val_loss: 0.4129 - val_accuracy: 0.8212\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3707 - accuracy: 0.8513 - val_loss: 0.4503 - val_accuracy: 0.8342\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3301 - accuracy: 0.8698 - val_loss: 0.4273 - val_accuracy: 0.8307\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3093 - accuracy: 0.8777 - val_loss: 0.4620 - val_accuracy: 0.7865\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2974 - accuracy: 0.8861 - val_loss: 0.4315 - val_accuracy: 0.8264\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2855 - accuracy: 0.8923 - val_loss: 0.4229 - val_accuracy: 0.8272\n",
            "params: {'batch_size': 16, 'dense_dim': 32, 'embed_dim': 256, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5071 - accuracy: 0.7925 - val_loss: 0.4469 - val_accuracy: 0.8147\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3698 - accuracy: 0.8506 - val_loss: 0.3992 - val_accuracy: 0.8309\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3305 - accuracy: 0.8713 - val_loss: 0.4237 - val_accuracy: 0.8179\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3082 - accuracy: 0.8810 - val_loss: 0.4029 - val_accuracy: 0.8317\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2962 - accuracy: 0.8874 - val_loss: 0.4267 - val_accuracy: 0.8331\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2809 - accuracy: 0.8942 - val_loss: 0.4450 - val_accuracy: 0.8286\n",
            "Epoch 7/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2717 - accuracy: 0.8978 - val_loss: 0.4387 - val_accuracy: 0.8112\n",
            "params: {'batch_size': 16, 'dense_dim': 32, 'embed_dim': 128, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.4958 - accuracy: 0.7894 - val_loss: 0.3996 - val_accuracy: 0.8358\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3701 - accuracy: 0.8518 - val_loss: 0.4026 - val_accuracy: 0.8337\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3310 - accuracy: 0.8705 - val_loss: 0.4006 - val_accuracy: 0.8366\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3068 - accuracy: 0.8826 - val_loss: 0.4212 - val_accuracy: 0.8348\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2933 - accuracy: 0.8896 - val_loss: 0.4245 - val_accuracy: 0.8282\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 10s 14ms/step - loss: 0.2855 - accuracy: 0.8919 - val_loss: 0.4362 - val_accuracy: 0.8177\n",
            "params: {'batch_size': 16, 'dense_dim': 32, 'embed_dim': 128, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5045 - accuracy: 0.7894 - val_loss: 0.4136 - val_accuracy: 0.8284\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3703 - accuracy: 0.8503 - val_loss: 0.4583 - val_accuracy: 0.8280\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3307 - accuracy: 0.8710 - val_loss: 0.4198 - val_accuracy: 0.8214\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3103 - accuracy: 0.8794 - val_loss: 0.4452 - val_accuracy: 0.8366\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2943 - accuracy: 0.8874 - val_loss: 0.4446 - val_accuracy: 0.8298\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2849 - accuracy: 0.8918 - val_loss: 0.4489 - val_accuracy: 0.8270\n",
            "params: {'batch_size': 16, 'dense_dim': 32, 'embed_dim': 128, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 12s 15ms/step - loss: 0.5054 - accuracy: 0.7810 - val_loss: 0.3966 - val_accuracy: 0.8309\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3668 - accuracy: 0.8500 - val_loss: 0.4013 - val_accuracy: 0.8305\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3310 - accuracy: 0.8690 - val_loss: 0.4424 - val_accuracy: 0.8064\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3147 - accuracy: 0.8737 - val_loss: 0.4460 - val_accuracy: 0.8339\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3010 - accuracy: 0.8828 - val_loss: 0.4499 - val_accuracy: 0.8157\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2864 - accuracy: 0.8915 - val_loss: 0.4964 - val_accuracy: 0.8335\n",
            "params: {'batch_size': 16, 'dense_dim': 64, 'embed_dim': 256, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5068 - accuracy: 0.7874 - val_loss: 0.3929 - val_accuracy: 0.8319\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3701 - accuracy: 0.8534 - val_loss: 0.4366 - val_accuracy: 0.8183\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 10s 13ms/step - loss: 0.3302 - accuracy: 0.8683 - val_loss: 0.4082 - val_accuracy: 0.8216\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3085 - accuracy: 0.8775 - val_loss: 0.4165 - val_accuracy: 0.8315\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2938 - accuracy: 0.8868 - val_loss: 0.4578 - val_accuracy: 0.8259\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2845 - accuracy: 0.8916 - val_loss: 0.4734 - val_accuracy: 0.8112\n",
            "params: {'batch_size': 16, 'dense_dim': 64, 'embed_dim': 256, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5130 - accuracy: 0.7847 - val_loss: 0.4116 - val_accuracy: 0.8220\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3675 - accuracy: 0.8503 - val_loss: 0.4055 - val_accuracy: 0.8313\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3303 - accuracy: 0.8724 - val_loss: 0.4540 - val_accuracy: 0.8122\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3063 - accuracy: 0.8824 - val_loss: 0.4174 - val_accuracy: 0.8346\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 10s 14ms/step - loss: 0.2892 - accuracy: 0.8884 - val_loss: 0.4213 - val_accuracy: 0.8183\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2805 - accuracy: 0.8968 - val_loss: 0.4167 - val_accuracy: 0.8243\n",
            "Epoch 7/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2698 - accuracy: 0.8966 - val_loss: 0.4394 - val_accuracy: 0.8331\n",
            "params: {'batch_size': 16, 'dense_dim': 64, 'embed_dim': 256, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5078 - accuracy: 0.7851 - val_loss: 0.4096 - val_accuracy: 0.8235\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3700 - accuracy: 0.8493 - val_loss: 0.4336 - val_accuracy: 0.8315\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3320 - accuracy: 0.8694 - val_loss: 0.4219 - val_accuracy: 0.8259\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3119 - accuracy: 0.8797 - val_loss: 0.4184 - val_accuracy: 0.8243\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2957 - accuracy: 0.8883 - val_loss: 0.4684 - val_accuracy: 0.7980\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2836 - accuracy: 0.8923 - val_loss: 0.4286 - val_accuracy: 0.8216\n",
            "params: {'batch_size': 16, 'dense_dim': 64, 'embed_dim': 128, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5153 - accuracy: 0.7844 - val_loss: 0.4385 - val_accuracy: 0.8235\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3707 - accuracy: 0.8463 - val_loss: 0.3936 - val_accuracy: 0.8284\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3342 - accuracy: 0.8650 - val_loss: 0.4513 - val_accuracy: 0.8321\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3091 - accuracy: 0.8810 - val_loss: 0.4189 - val_accuracy: 0.8300\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2929 - accuracy: 0.8889 - val_loss: 0.4397 - val_accuracy: 0.8261\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2791 - accuracy: 0.8960 - val_loss: 0.4254 - val_accuracy: 0.8140\n",
            "Epoch 7/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.2701 - accuracy: 0.8978 - val_loss: 0.4525 - val_accuracy: 0.8118\n",
            "params: {'batch_size': 16, 'dense_dim': 64, 'embed_dim': 128, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5138 - accuracy: 0.7852 - val_loss: 0.5345 - val_accuracy: 0.8058\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 12ms/step - loss: 0.3697 - accuracy: 0.8523 - val_loss: 0.3994 - val_accuracy: 0.8321\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 10s 14ms/step - loss: 0.3301 - accuracy: 0.8685 - val_loss: 0.4561 - val_accuracy: 0.8356\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3109 - accuracy: 0.8767 - val_loss: 0.4227 - val_accuracy: 0.8155\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2974 - accuracy: 0.8871 - val_loss: 0.4444 - val_accuracy: 0.8081\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2845 - accuracy: 0.8925 - val_loss: 0.4608 - val_accuracy: 0.8272\n",
            "Epoch 7/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2773 - accuracy: 0.8956 - val_loss: 0.4773 - val_accuracy: 0.8085\n",
            "params: {'batch_size': 16, 'dense_dim': 64, 'embed_dim': 128, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "711/711 [==============================] - 11s 13ms/step - loss: 0.5113 - accuracy: 0.7860 - val_loss: 0.4170 - val_accuracy: 0.8282\n",
            "Epoch 2/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3689 - accuracy: 0.8537 - val_loss: 0.4035 - val_accuracy: 0.8288\n",
            "Epoch 3/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3290 - accuracy: 0.8703 - val_loss: 0.4921 - val_accuracy: 0.8188\n",
            "Epoch 4/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.3090 - accuracy: 0.8804 - val_loss: 0.4088 - val_accuracy: 0.8354\n",
            "Epoch 5/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2919 - accuracy: 0.8905 - val_loss: 0.4125 - val_accuracy: 0.8270\n",
            "Epoch 6/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2821 - accuracy: 0.8957 - val_loss: 0.4379 - val_accuracy: 0.8348\n",
            "Epoch 7/30\n",
            "711/711 [==============================] - 9s 13ms/step - loss: 0.2702 - accuracy: 0.9004 - val_loss: 0.4569 - val_accuracy: 0.8231\n",
            "params: {'batch_size': 32, 'dense_dim': 16, 'embed_dim': 256, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 21ms/step - loss: 0.5297 - accuracy: 0.7799 - val_loss: 0.4174 - val_accuracy: 0.8138\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3584 - accuracy: 0.8525 - val_loss: 0.4154 - val_accuracy: 0.8300\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3139 - accuracy: 0.8733 - val_loss: 0.4299 - val_accuracy: 0.8296\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2817 - accuracy: 0.8875 - val_loss: 0.4530 - val_accuracy: 0.8305\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2615 - accuracy: 0.8992 - val_loss: 0.4620 - val_accuracy: 0.7931\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2459 - accuracy: 0.9027 - val_loss: 0.4672 - val_accuracy: 0.8216\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2320 - accuracy: 0.9107 - val_loss: 0.4977 - val_accuracy: 0.8255\n",
            "params: {'batch_size': 32, 'dense_dim': 16, 'embed_dim': 256, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 10s 21ms/step - loss: 0.5244 - accuracy: 0.7840 - val_loss: 0.4145 - val_accuracy: 0.8251\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3551 - accuracy: 0.8531 - val_loss: 0.5324 - val_accuracy: 0.7625\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3105 - accuracy: 0.8774 - val_loss: 0.4629 - val_accuracy: 0.8179\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2805 - accuracy: 0.8922 - val_loss: 0.4463 - val_accuracy: 0.8319\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2628 - accuracy: 0.8986 - val_loss: 0.4312 - val_accuracy: 0.8264\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2487 - accuracy: 0.9030 - val_loss: 0.4442 - val_accuracy: 0.8233\n",
            "params: {'batch_size': 32, 'dense_dim': 16, 'embed_dim': 256, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 22ms/step - loss: 0.5235 - accuracy: 0.7752 - val_loss: 0.4576 - val_accuracy: 0.8069\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.3532 - accuracy: 0.8578 - val_loss: 0.4326 - val_accuracy: 0.8376\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3092 - accuracy: 0.8768 - val_loss: 0.4153 - val_accuracy: 0.8339\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 8s 21ms/step - loss: 0.2806 - accuracy: 0.8911 - val_loss: 0.4515 - val_accuracy: 0.8311\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2619 - accuracy: 0.9011 - val_loss: 0.4359 - val_accuracy: 0.8251\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2462 - accuracy: 0.9066 - val_loss: 0.4624 - val_accuracy: 0.8243\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2325 - accuracy: 0.9125 - val_loss: 0.4435 - val_accuracy: 0.8181\n",
            "Epoch 8/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2211 - accuracy: 0.9190 - val_loss: 0.5265 - val_accuracy: 0.8128\n",
            "params: {'batch_size': 32, 'dense_dim': 16, 'embed_dim': 128, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 21ms/step - loss: 0.5277 - accuracy: 0.7775 - val_loss: 0.3941 - val_accuracy: 0.8278\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3583 - accuracy: 0.8564 - val_loss: 0.4432 - val_accuracy: 0.8315\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3088 - accuracy: 0.8774 - val_loss: 0.4287 - val_accuracy: 0.8218\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2795 - accuracy: 0.8911 - val_loss: 0.4361 - val_accuracy: 0.8309\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2604 - accuracy: 0.8996 - val_loss: 0.4764 - val_accuracy: 0.8241\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2440 - accuracy: 0.9058 - val_loss: 0.4959 - val_accuracy: 0.8106\n",
            "params: {'batch_size': 32, 'dense_dim': 16, 'embed_dim': 128, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 21ms/step - loss: 0.5423 - accuracy: 0.7755 - val_loss: 0.4300 - val_accuracy: 0.8036\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3577 - accuracy: 0.8524 - val_loss: 0.4626 - val_accuracy: 0.8348\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3101 - accuracy: 0.8780 - val_loss: 0.4440 - val_accuracy: 0.8044\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2828 - accuracy: 0.8905 - val_loss: 0.4616 - val_accuracy: 0.8333\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2611 - accuracy: 0.8984 - val_loss: 0.5224 - val_accuracy: 0.8288\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2471 - accuracy: 0.9037 - val_loss: 0.4787 - val_accuracy: 0.8280\n",
            "params: {'batch_size': 32, 'dense_dim': 16, 'embed_dim': 128, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 22ms/step - loss: 0.5289 - accuracy: 0.7815 - val_loss: 0.4218 - val_accuracy: 0.8243\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3626 - accuracy: 0.8502 - val_loss: 0.3928 - val_accuracy: 0.8339\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3100 - accuracy: 0.8740 - val_loss: 0.5585 - val_accuracy: 0.8350\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2848 - accuracy: 0.8889 - val_loss: 0.4454 - val_accuracy: 0.8040\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2581 - accuracy: 0.8996 - val_loss: 0.4507 - val_accuracy: 0.8288\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2445 - accuracy: 0.9063 - val_loss: 0.4521 - val_accuracy: 0.8237\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2334 - accuracy: 0.9140 - val_loss: 0.4595 - val_accuracy: 0.8212\n",
            "params: {'batch_size': 32, 'dense_dim': 32, 'embed_dim': 256, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 21ms/step - loss: 0.5322 - accuracy: 0.7739 - val_loss: 0.3951 - val_accuracy: 0.8274\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3576 - accuracy: 0.8546 - val_loss: 0.4117 - val_accuracy: 0.8348\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.3132 - accuracy: 0.8749 - val_loss: 0.3920 - val_accuracy: 0.8370\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2827 - accuracy: 0.8905 - val_loss: 0.4955 - val_accuracy: 0.8116\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2625 - accuracy: 0.8984 - val_loss: 0.4462 - val_accuracy: 0.8317\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 8s 21ms/step - loss: 0.2432 - accuracy: 0.9066 - val_loss: 0.4983 - val_accuracy: 0.8251\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2315 - accuracy: 0.9151 - val_loss: 0.4741 - val_accuracy: 0.8038\n",
            "Epoch 8/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2184 - accuracy: 0.9190 - val_loss: 0.5489 - val_accuracy: 0.8251\n",
            "params: {'batch_size': 32, 'dense_dim': 32, 'embed_dim': 256, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 22ms/step - loss: 0.5179 - accuracy: 0.7782 - val_loss: 0.4052 - val_accuracy: 0.8342\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3534 - accuracy: 0.8540 - val_loss: 0.4267 - val_accuracy: 0.8346\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3076 - accuracy: 0.8765 - val_loss: 0.4251 - val_accuracy: 0.8181\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2821 - accuracy: 0.8896 - val_loss: 0.4237 - val_accuracy: 0.8239\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2621 - accuracy: 0.9001 - val_loss: 0.4532 - val_accuracy: 0.8259\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2531 - accuracy: 0.9040 - val_loss: 0.4369 - val_accuracy: 0.8264\n",
            "params: {'batch_size': 32, 'dense_dim': 32, 'embed_dim': 256, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 21ms/step - loss: 0.5270 - accuracy: 0.7828 - val_loss: 0.5438 - val_accuracy: 0.8021\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3642 - accuracy: 0.8546 - val_loss: 0.3894 - val_accuracy: 0.8348\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3183 - accuracy: 0.8741 - val_loss: 0.4162 - val_accuracy: 0.8360\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2858 - accuracy: 0.8870 - val_loss: 0.5349 - val_accuracy: 0.7833\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2657 - accuracy: 0.8988 - val_loss: 0.4763 - val_accuracy: 0.8294\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2419 - accuracy: 0.9084 - val_loss: 0.4452 - val_accuracy: 0.8243\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2267 - accuracy: 0.9177 - val_loss: 0.4751 - val_accuracy: 0.8268\n",
            "params: {'batch_size': 32, 'dense_dim': 32, 'embed_dim': 128, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 22ms/step - loss: 0.5196 - accuracy: 0.7776 - val_loss: 0.4175 - val_accuracy: 0.8235\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.3577 - accuracy: 0.8516 - val_loss: 0.3942 - val_accuracy: 0.8346\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.3124 - accuracy: 0.8760 - val_loss: 0.4242 - val_accuracy: 0.8319\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2840 - accuracy: 0.8886 - val_loss: 0.4765 - val_accuracy: 0.8149\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2616 - accuracy: 0.9007 - val_loss: 0.4427 - val_accuracy: 0.8198\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2495 - accuracy: 0.9053 - val_loss: 0.4869 - val_accuracy: 0.8253\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2317 - accuracy: 0.9141 - val_loss: 0.5745 - val_accuracy: 0.7613\n",
            "params: {'batch_size': 32, 'dense_dim': 32, 'embed_dim': 128, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 21ms/step - loss: 0.5268 - accuracy: 0.7789 - val_loss: 0.4866 - val_accuracy: 0.8163\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3585 - accuracy: 0.8535 - val_loss: 0.4515 - val_accuracy: 0.8331\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 8s 21ms/step - loss: 0.3094 - accuracy: 0.8778 - val_loss: 0.4192 - val_accuracy: 0.8333\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2818 - accuracy: 0.8893 - val_loss: 0.4126 - val_accuracy: 0.8264\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2582 - accuracy: 0.9014 - val_loss: 0.4093 - val_accuracy: 0.8264\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2484 - accuracy: 0.9052 - val_loss: 0.5131 - val_accuracy: 0.7923\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2318 - accuracy: 0.9123 - val_loss: 0.5251 - val_accuracy: 0.7878\n",
            "Epoch 8/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2189 - accuracy: 0.9201 - val_loss: 0.4673 - val_accuracy: 0.8276\n",
            "Epoch 9/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2046 - accuracy: 0.9260 - val_loss: 0.5309 - val_accuracy: 0.8200\n",
            "Epoch 10/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.1952 - accuracy: 0.9296 - val_loss: 0.5507 - val_accuracy: 0.8095\n",
            "params: {'batch_size': 32, 'dense_dim': 32, 'embed_dim': 128, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 21ms/step - loss: 0.5346 - accuracy: 0.7755 - val_loss: 0.4076 - val_accuracy: 0.8218\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.3614 - accuracy: 0.8519 - val_loss: 0.4265 - val_accuracy: 0.8257\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.3126 - accuracy: 0.8727 - val_loss: 0.4026 - val_accuracy: 0.8264\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2846 - accuracy: 0.8912 - val_loss: 0.4668 - val_accuracy: 0.8282\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2631 - accuracy: 0.8980 - val_loss: 0.5433 - val_accuracy: 0.8274\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2488 - accuracy: 0.9057 - val_loss: 0.4942 - val_accuracy: 0.8169\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2346 - accuracy: 0.9112 - val_loss: 0.4779 - val_accuracy: 0.8087\n",
            "Epoch 8/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2221 - accuracy: 0.9192 - val_loss: 0.5524 - val_accuracy: 0.8165\n",
            "params: {'batch_size': 32, 'dense_dim': 64, 'embed_dim': 256, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 21ms/step - loss: 0.5289 - accuracy: 0.7762 - val_loss: 0.4310 - val_accuracy: 0.8204\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3561 - accuracy: 0.8541 - val_loss: 0.4293 - val_accuracy: 0.8366\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3121 - accuracy: 0.8744 - val_loss: 0.4347 - val_accuracy: 0.8323\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2785 - accuracy: 0.8905 - val_loss: 0.5072 - val_accuracy: 0.8241\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2615 - accuracy: 0.9012 - val_loss: 0.4911 - val_accuracy: 0.8255\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2479 - accuracy: 0.9088 - val_loss: 0.4793 - val_accuracy: 0.8157\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2301 - accuracy: 0.9150 - val_loss: 0.5155 - val_accuracy: 0.8247\n",
            "params: {'batch_size': 32, 'dense_dim': 64, 'embed_dim': 256, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 21ms/step - loss: 0.5192 - accuracy: 0.7797 - val_loss: 0.6202 - val_accuracy: 0.6679\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.3600 - accuracy: 0.8537 - val_loss: 0.3953 - val_accuracy: 0.8317\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3128 - accuracy: 0.8747 - val_loss: 0.4144 - val_accuracy: 0.8356\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2818 - accuracy: 0.8887 - val_loss: 0.4318 - val_accuracy: 0.8317\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2629 - accuracy: 0.9000 - val_loss: 0.4450 - val_accuracy: 0.8284\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2470 - accuracy: 0.9047 - val_loss: 0.4398 - val_accuracy: 0.8245\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2353 - accuracy: 0.9101 - val_loss: 0.5132 - val_accuracy: 0.7974\n",
            "params: {'batch_size': 32, 'dense_dim': 64, 'embed_dim': 256, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 22ms/step - loss: 0.5212 - accuracy: 0.7752 - val_loss: 0.4418 - val_accuracy: 0.8204\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 8s 21ms/step - loss: 0.3586 - accuracy: 0.8525 - val_loss: 0.4911 - val_accuracy: 0.7986\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3081 - accuracy: 0.8747 - val_loss: 0.4415 - val_accuracy: 0.8358\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2794 - accuracy: 0.8919 - val_loss: 0.4167 - val_accuracy: 0.8348\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2621 - accuracy: 0.8979 - val_loss: 0.5220 - val_accuracy: 0.8266\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2456 - accuracy: 0.9068 - val_loss: 0.5203 - val_accuracy: 0.8270\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2308 - accuracy: 0.9152 - val_loss: 0.5085 - val_accuracy: 0.8095\n",
            "Epoch 8/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2152 - accuracy: 0.9195 - val_loss: 0.4456 - val_accuracy: 0.8149\n",
            "Epoch 9/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2015 - accuracy: 0.9264 - val_loss: 0.6208 - val_accuracy: 0.8167\n",
            "params: {'batch_size': 32, 'dense_dim': 64, 'embed_dim': 128, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 10s 22ms/step - loss: 0.5241 - accuracy: 0.7816 - val_loss: 0.4060 - val_accuracy: 0.8298\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.3568 - accuracy: 0.8573 - val_loss: 0.4254 - val_accuracy: 0.8253\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3141 - accuracy: 0.8729 - val_loss: 0.5718 - val_accuracy: 0.7369\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2814 - accuracy: 0.8875 - val_loss: 0.4459 - val_accuracy: 0.8331\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2569 - accuracy: 0.9019 - val_loss: 0.4287 - val_accuracy: 0.8270\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2380 - accuracy: 0.9099 - val_loss: 0.4717 - val_accuracy: 0.8259\n",
            "params: {'batch_size': 32, 'dense_dim': 64, 'embed_dim': 128, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 21ms/step - loss: 0.5211 - accuracy: 0.7853 - val_loss: 0.7659 - val_accuracy: 0.5883\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3583 - accuracy: 0.8516 - val_loss: 0.5468 - val_accuracy: 0.7627\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3129 - accuracy: 0.8755 - val_loss: 0.4401 - val_accuracy: 0.8339\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2831 - accuracy: 0.8905 - val_loss: 0.4117 - val_accuracy: 0.8165\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2608 - accuracy: 0.8985 - val_loss: 0.4462 - val_accuracy: 0.8206\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2456 - accuracy: 0.9051 - val_loss: 0.4949 - val_accuracy: 0.8225\n",
            "Epoch 7/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2324 - accuracy: 0.9135 - val_loss: 0.4952 - val_accuracy: 0.8163\n",
            "Epoch 8/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2185 - accuracy: 0.9190 - val_loss: 0.5225 - val_accuracy: 0.8126\n",
            "Epoch 9/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2105 - accuracy: 0.9226 - val_loss: 0.4966 - val_accuracy: 0.8175\n",
            "params: {'batch_size': 32, 'dense_dim': 64, 'embed_dim': 128, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "356/356 [==============================] - 9s 21ms/step - loss: 0.5393 - accuracy: 0.7753 - val_loss: 0.4192 - val_accuracy: 0.8327\n",
            "Epoch 2/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3590 - accuracy: 0.8553 - val_loss: 0.4226 - val_accuracy: 0.8282\n",
            "Epoch 3/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.3090 - accuracy: 0.8758 - val_loss: 0.4194 - val_accuracy: 0.8241\n",
            "Epoch 4/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2799 - accuracy: 0.8930 - val_loss: 0.4461 - val_accuracy: 0.8225\n",
            "Epoch 5/30\n",
            "356/356 [==============================] - 7s 21ms/step - loss: 0.2629 - accuracy: 0.9004 - val_loss: 0.4238 - val_accuracy: 0.8261\n",
            "Epoch 6/30\n",
            "356/356 [==============================] - 7s 20ms/step - loss: 0.2475 - accuracy: 0.9069 - val_loss: 0.5042 - val_accuracy: 0.8272\n",
            "params: {'batch_size': 128, 'dense_dim': 16, 'embed_dim': 256, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6515 - accuracy: 0.7422 - val_loss: 0.4010 - val_accuracy: 0.8319\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.3660 - accuracy: 0.8472 - val_loss: 0.4597 - val_accuracy: 0.8192\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2938 - accuracy: 0.8754 - val_loss: 0.4514 - val_accuracy: 0.8327\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2428 - accuracy: 0.8955 - val_loss: 0.4686 - val_accuracy: 0.8225\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2124 - accuracy: 0.9103 - val_loss: 0.4740 - val_accuracy: 0.8048\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1809 - accuracy: 0.9205 - val_loss: 0.5301 - val_accuracy: 0.8214\n",
            "params: {'batch_size': 128, 'dense_dim': 16, 'embed_dim': 256, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6886 - accuracy: 0.7405 - val_loss: 0.4568 - val_accuracy: 0.8071\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.3805 - accuracy: 0.8398 - val_loss: 0.4027 - val_accuracy: 0.8255\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2995 - accuracy: 0.8729 - val_loss: 0.4141 - val_accuracy: 0.8393\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 7s 76ms/step - loss: 0.2431 - accuracy: 0.8960 - val_loss: 0.4759 - val_accuracy: 0.8374\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2041 - accuracy: 0.9145 - val_loss: 0.4605 - val_accuracy: 0.8235\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1741 - accuracy: 0.9248 - val_loss: 0.4984 - val_accuracy: 0.8214\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1394 - accuracy: 0.9432 - val_loss: 0.6864 - val_accuracy: 0.8245\n",
            "params: {'batch_size': 128, 'dense_dim': 16, 'embed_dim': 256, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6692 - accuracy: 0.7388 - val_loss: 0.4000 - val_accuracy: 0.8309\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3725 - accuracy: 0.8473 - val_loss: 0.3874 - val_accuracy: 0.8346\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2993 - accuracy: 0.8737 - val_loss: 0.4053 - val_accuracy: 0.8282\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2438 - accuracy: 0.8949 - val_loss: 0.4663 - val_accuracy: 0.8237\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2110 - accuracy: 0.9090 - val_loss: 0.5231 - val_accuracy: 0.8276\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1798 - accuracy: 0.9212 - val_loss: 0.5987 - val_accuracy: 0.8190\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1593 - accuracy: 0.9317 - val_loss: 0.6255 - val_accuracy: 0.8292\n",
            "params: {'batch_size': 128, 'dense_dim': 16, 'embed_dim': 128, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 73ms/step - loss: 0.7257 - accuracy: 0.7214 - val_loss: 0.3970 - val_accuracy: 0.8307\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3913 - accuracy: 0.8341 - val_loss: 0.3795 - val_accuracy: 0.8366\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2990 - accuracy: 0.8735 - val_loss: 0.4290 - val_accuracy: 0.8222\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.2413 - accuracy: 0.8981 - val_loss: 0.4480 - val_accuracy: 0.8212\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 7s 75ms/step - loss: 0.2051 - accuracy: 0.9139 - val_loss: 0.5518 - val_accuracy: 0.8264\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.1699 - accuracy: 0.9262 - val_loss: 0.5307 - val_accuracy: 0.8303\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1343 - accuracy: 0.9429 - val_loss: 0.5659 - val_accuracy: 0.8216\n",
            "params: {'batch_size': 128, 'dense_dim': 16, 'embed_dim': 128, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6899 - accuracy: 0.7337 - val_loss: 0.4132 - val_accuracy: 0.8161\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3773 - accuracy: 0.8411 - val_loss: 0.4549 - val_accuracy: 0.8186\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2971 - accuracy: 0.8748 - val_loss: 0.4045 - val_accuracy: 0.8376\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 70ms/step - loss: 0.2475 - accuracy: 0.8968 - val_loss: 0.4114 - val_accuracy: 0.8348\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2077 - accuracy: 0.9125 - val_loss: 0.4426 - val_accuracy: 0.8346\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1684 - accuracy: 0.9295 - val_loss: 0.5002 - val_accuracy: 0.8138\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1426 - accuracy: 0.9403 - val_loss: 0.6461 - val_accuracy: 0.8210\n",
            "Epoch 8/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1130 - accuracy: 0.9531 - val_loss: 0.6750 - val_accuracy: 0.7635\n",
            "params: {'batch_size': 128, 'dense_dim': 16, 'embed_dim': 128, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.7206 - accuracy: 0.7278 - val_loss: 0.3953 - val_accuracy: 0.8235\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3796 - accuracy: 0.8421 - val_loss: 0.3836 - val_accuracy: 0.8387\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2961 - accuracy: 0.8768 - val_loss: 0.4260 - val_accuracy: 0.8249\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2464 - accuracy: 0.8978 - val_loss: 0.4445 - val_accuracy: 0.8317\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2080 - accuracy: 0.9131 - val_loss: 0.4693 - val_accuracy: 0.8300\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1731 - accuracy: 0.9242 - val_loss: 0.5207 - val_accuracy: 0.8181\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.1431 - accuracy: 0.9428 - val_loss: 0.5385 - val_accuracy: 0.8303\n",
            "params: {'batch_size': 128, 'dense_dim': 32, 'embed_dim': 256, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6562 - accuracy: 0.7320 - val_loss: 0.4036 - val_accuracy: 0.8280\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3798 - accuracy: 0.8396 - val_loss: 0.3844 - val_accuracy: 0.8387\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2974 - accuracy: 0.8744 - val_loss: 0.3995 - val_accuracy: 0.8405\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2468 - accuracy: 0.8961 - val_loss: 0.4870 - val_accuracy: 0.8167\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2042 - accuracy: 0.9103 - val_loss: 0.6168 - val_accuracy: 0.8300\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1736 - accuracy: 0.9267 - val_loss: 0.4878 - val_accuracy: 0.8344\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1452 - accuracy: 0.9401 - val_loss: 0.5074 - val_accuracy: 0.8222\n",
            "params: {'batch_size': 128, 'dense_dim': 32, 'embed_dim': 256, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6412 - accuracy: 0.7410 - val_loss: 0.3926 - val_accuracy: 0.8286\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3710 - accuracy: 0.8445 - val_loss: 0.5404 - val_accuracy: 0.7769\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2924 - accuracy: 0.8736 - val_loss: 0.4239 - val_accuracy: 0.8134\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2479 - accuracy: 0.8924 - val_loss: 0.4182 - val_accuracy: 0.8333\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2079 - accuracy: 0.9103 - val_loss: 0.5489 - val_accuracy: 0.8286\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1820 - accuracy: 0.9226 - val_loss: 0.5766 - val_accuracy: 0.8218\n",
            "params: {'batch_size': 128, 'dense_dim': 32, 'embed_dim': 256, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6628 - accuracy: 0.7393 - val_loss: 0.4074 - val_accuracy: 0.8175\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.3771 - accuracy: 0.8398 - val_loss: 0.3921 - val_accuracy: 0.8313\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2937 - accuracy: 0.8737 - val_loss: 0.3907 - val_accuracy: 0.8323\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2493 - accuracy: 0.8953 - val_loss: 0.3967 - val_accuracy: 0.8335\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2120 - accuracy: 0.9103 - val_loss: 0.4169 - val_accuracy: 0.8292\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1786 - accuracy: 0.9224 - val_loss: 0.5732 - val_accuracy: 0.7888\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1529 - accuracy: 0.9360 - val_loss: 0.5828 - val_accuracy: 0.8243\n",
            "Epoch 8/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1220 - accuracy: 0.9477 - val_loss: 0.7446 - val_accuracy: 0.8192\n",
            "params: {'batch_size': 128, 'dense_dim': 32, 'embed_dim': 128, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.7008 - accuracy: 0.7346 - val_loss: 0.3975 - val_accuracy: 0.8309\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3776 - accuracy: 0.8424 - val_loss: 0.4333 - val_accuracy: 0.8339\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3015 - accuracy: 0.8744 - val_loss: 0.3969 - val_accuracy: 0.8376\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2455 - accuracy: 0.8982 - val_loss: 0.4835 - val_accuracy: 0.8294\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2076 - accuracy: 0.9139 - val_loss: 0.5274 - val_accuracy: 0.8319\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1810 - accuracy: 0.9233 - val_loss: 0.5957 - val_accuracy: 0.7730\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1475 - accuracy: 0.9388 - val_loss: 0.6427 - val_accuracy: 0.8270\n",
            "Epoch 8/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1231 - accuracy: 0.9498 - val_loss: 0.6038 - val_accuracy: 0.8122\n",
            "params: {'batch_size': 128, 'dense_dim': 32, 'embed_dim': 128, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6688 - accuracy: 0.7359 - val_loss: 0.4032 - val_accuracy: 0.8270\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3734 - accuracy: 0.8430 - val_loss: 0.4101 - val_accuracy: 0.8294\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.2996 - accuracy: 0.8740 - val_loss: 0.3996 - val_accuracy: 0.8391\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.2469 - accuracy: 0.8951 - val_loss: 0.4268 - val_accuracy: 0.8056\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.2150 - accuracy: 0.9072 - val_loss: 0.5655 - val_accuracy: 0.8331\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.1816 - accuracy: 0.9208 - val_loss: 0.5887 - val_accuracy: 0.8294\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1549 - accuracy: 0.9338 - val_loss: 0.5396 - val_accuracy: 0.8266\n",
            "Epoch 8/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1295 - accuracy: 0.9443 - val_loss: 0.7143 - val_accuracy: 0.8208\n",
            "params: {'batch_size': 128, 'dense_dim': 32, 'embed_dim': 128, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6677 - accuracy: 0.7389 - val_loss: 0.4856 - val_accuracy: 0.8079\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3713 - accuracy: 0.8399 - val_loss: 0.4016 - val_accuracy: 0.8397\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2963 - accuracy: 0.8737 - val_loss: 0.4019 - val_accuracy: 0.8346\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 70ms/step - loss: 0.2440 - accuracy: 0.8954 - val_loss: 0.4256 - val_accuracy: 0.8364\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2127 - accuracy: 0.9090 - val_loss: 0.4687 - val_accuracy: 0.8268\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1739 - accuracy: 0.9246 - val_loss: 0.7251 - val_accuracy: 0.8294\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1527 - accuracy: 0.9352 - val_loss: 0.5778 - val_accuracy: 0.8220\n",
            "params: {'batch_size': 128, 'dense_dim': 64, 'embed_dim': 256, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6816 - accuracy: 0.7264 - val_loss: 0.4049 - val_accuracy: 0.8280\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3772 - accuracy: 0.8400 - val_loss: 0.4787 - val_accuracy: 0.8173\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2984 - accuracy: 0.8772 - val_loss: 0.4138 - val_accuracy: 0.8239\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2499 - accuracy: 0.8950 - val_loss: 0.4500 - val_accuracy: 0.8350\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2171 - accuracy: 0.9047 - val_loss: 0.4639 - val_accuracy: 0.8247\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1768 - accuracy: 0.9233 - val_loss: 0.5594 - val_accuracy: 0.7945\n",
            "params: {'batch_size': 128, 'dense_dim': 64, 'embed_dim': 256, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6827 - accuracy: 0.7372 - val_loss: 0.4013 - val_accuracy: 0.8317\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3761 - accuracy: 0.8429 - val_loss: 0.3886 - val_accuracy: 0.8333\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2967 - accuracy: 0.8738 - val_loss: 0.5299 - val_accuracy: 0.8202\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2471 - accuracy: 0.8935 - val_loss: 0.4284 - val_accuracy: 0.8106\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2123 - accuracy: 0.9086 - val_loss: 0.5719 - val_accuracy: 0.8237\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1802 - accuracy: 0.9235 - val_loss: 0.5341 - val_accuracy: 0.8331\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1502 - accuracy: 0.9351 - val_loss: 0.5348 - val_accuracy: 0.8021\n",
            "params: {'batch_size': 128, 'dense_dim': 64, 'embed_dim': 256, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6376 - accuracy: 0.7398 - val_loss: 0.5033 - val_accuracy: 0.7660\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3782 - accuracy: 0.8398 - val_loss: 0.3776 - val_accuracy: 0.8401\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2904 - accuracy: 0.8783 - val_loss: 0.4429 - val_accuracy: 0.8381\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2465 - accuracy: 0.8968 - val_loss: 0.4695 - val_accuracy: 0.8225\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2111 - accuracy: 0.9065 - val_loss: 0.5303 - val_accuracy: 0.8266\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1841 - accuracy: 0.9210 - val_loss: 0.5760 - val_accuracy: 0.8214\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1631 - accuracy: 0.9300 - val_loss: 0.5628 - val_accuracy: 0.8231\n",
            "params: {'batch_size': 128, 'dense_dim': 64, 'embed_dim': 128, 'num_heads': 2}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6839 - accuracy: 0.7381 - val_loss: 0.3979 - val_accuracy: 0.8268\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3781 - accuracy: 0.8433 - val_loss: 0.4530 - val_accuracy: 0.8257\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2989 - accuracy: 0.8716 - val_loss: 0.4771 - val_accuracy: 0.8362\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2468 - accuracy: 0.8939 - val_loss: 0.5203 - val_accuracy: 0.7947\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2140 - accuracy: 0.9066 - val_loss: 0.5097 - val_accuracy: 0.7984\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1869 - accuracy: 0.9205 - val_loss: 0.5260 - val_accuracy: 0.8079\n",
            "params: {'batch_size': 128, 'dense_dim': 64, 'embed_dim': 128, 'num_heads': 3}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 75ms/step - loss: 0.6440 - accuracy: 0.7369 - val_loss: 0.4624 - val_accuracy: 0.8064\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3746 - accuracy: 0.8406 - val_loss: 0.4089 - val_accuracy: 0.8350\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2894 - accuracy: 0.8788 - val_loss: 0.4077 - val_accuracy: 0.8370\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2508 - accuracy: 0.8930 - val_loss: 0.4509 - val_accuracy: 0.8116\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2104 - accuracy: 0.9088 - val_loss: 0.4311 - val_accuracy: 0.8311\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1848 - accuracy: 0.9209 - val_loss: 0.5236 - val_accuracy: 0.8290\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1579 - accuracy: 0.9326 - val_loss: 0.6321 - val_accuracy: 0.8153\n",
            "Epoch 8/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1348 - accuracy: 0.9405 - val_loss: 0.6129 - val_accuracy: 0.8126\n",
            "params: {'batch_size': 128, 'dense_dim': 64, 'embed_dim': 128, 'num_heads': 6}\n",
            "Epoch 1/30\n",
            "89/89 [==============================] - 8s 74ms/step - loss: 0.6849 - accuracy: 0.7351 - val_loss: 0.5163 - val_accuracy: 0.7857\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.3834 - accuracy: 0.8387 - val_loss: 0.4396 - val_accuracy: 0.8214\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.2964 - accuracy: 0.8750 - val_loss: 0.4059 - val_accuracy: 0.8389\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.2464 - accuracy: 0.8966 - val_loss: 0.4830 - val_accuracy: 0.8354\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.2054 - accuracy: 0.9125 - val_loss: 0.4676 - val_accuracy: 0.8270\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 6s 72ms/step - loss: 0.1754 - accuracy: 0.9241 - val_loss: 0.4984 - val_accuracy: 0.8259\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1379 - accuracy: 0.9440 - val_loss: 0.5912 - val_accuracy: 0.8268\n",
            "Epoch 8/30\n",
            "89/89 [==============================] - 6s 71ms/step - loss: 0.1178 - accuracy: 0.9529 - val_loss: 0.5812 - val_accuracy: 0.8266\n"
          ]
        }
      ],
      "source": [
        "param_grid = {'batch_size': [16,32,128],'embed_dim': [256,128], 'num_heads': [2,3,6], 'dense_dim' : [16,32,64]}\n",
        "results = grid_search(param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgKRoPuFM8Vy"
      },
      "outputs": [],
      "source": [
        "def scores_to_dataframe(scores):\n",
        "    \"\"\" Return hyperparameters and scores in a data frame. \"\"\"\n",
        "    \n",
        "    params_list, acc_list = scores\n",
        "    \n",
        "    params = pd.DataFrame(params_list)\n",
        "    accs = pd.Series(acc_list)\n",
        "    scores_df = pd.concat([params, accs], axis=1)\n",
        "    \n",
        "    return scores_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spIf-ZOCM-Hn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4369b262-68ed-4ae2-8037-db6ed03df4a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    batch_size  embed_dim  num_heads  dense_dim         0\n",
              "0           16        256          2         16  0.824405\n",
              "1           16        256          3         16  0.827278\n",
              "2           16        256          6         16  0.830768\n",
              "3           16        128          2         16  0.822660\n",
              "4           16        128          3         16  0.825739\n",
              "5           16        128          6         16  0.819273\n",
              "6           16        256          2         32  0.829844\n",
              "7           16        256          3         32  0.826765\n",
              "8           16        256          6         32  0.819889\n",
              "9           16        128          2         32  0.822968\n",
              "10          16        128          3         32  0.828407\n",
              "11          16        128          6         32  0.824610\n",
              "12          16        256          2         64  0.818555\n",
              "13          16        256          3         64  0.828715\n",
              "14          16        256          6         64  0.809832\n",
              "15          16        128          2         64  0.812910\n",
              "16          16        128          3         64  0.817837\n",
              "17          16        128          6         64  0.828920\n",
              "18          32        256          2         16  0.823584\n",
              "19          32        256          3         16  0.824815\n",
              "20          32        256          6         16  0.815476\n",
              "21          32        128          2         16  0.817323\n",
              "22          32        128          3         16  0.828407\n",
              "23          32        128          6         16  0.822455\n",
              "24          32        256          2         32  0.814450\n",
              "25          32        256          3         32  0.826149\n",
              "26          32        256          6         32  0.825534\n",
              "27          32        128          2         32  0.793309\n",
              "28          32        128          3         32  0.814758\n",
              "29          32        128          6         32  0.812603\n",
              "30          32        256          2         64  0.820197\n",
              "31          32        256          3         64  0.810961\n",
              "32          32        256          6         64  0.815784\n",
              "33          32        128          2         64  0.826457\n",
              "34          32        128          3         64  0.815066\n",
              "35          32        128          6         64  0.826663\n",
              "36         128        256          2         16  0.813116\n",
              "37         128        256          3         16  0.822968\n",
              "38         128        256          6         16  0.824097\n",
              "39         128        128          2         16  0.825944\n",
              "40         128        128          3         16  0.792282\n",
              "41         128        128          6         16  0.824199\n",
              "42         128        256          2         32  0.828305\n",
              "43         128        256          3         32  0.825226\n",
              "44         128        256          6         32  0.821736\n",
              "45         128        128          2         32  0.819581\n",
              "46         128        128          3         32  0.823686\n",
              "47         128        128          6         32  0.825739\n",
              "48         128        256          2         64  0.809626\n",
              "49         128        256          3         64  0.817631\n",
              "50         128        256          6         64  0.822250\n",
              "51         128        128          2         64  0.803161\n",
              "52         128        128          3         64  0.813937\n",
              "53         128        128          6         64  0.826663"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff136205-21ef-4d74-a30a-bd880ec30b3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>batch_size</th>\n",
              "      <th>embed_dim</th>\n",
              "      <th>num_heads</th>\n",
              "      <th>dense_dim</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.824405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>0.827278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16</td>\n",
              "      <td>256</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>0.830768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.822660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16</td>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>0.825739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>16</td>\n",
              "      <td>128</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>0.819273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>16</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.829844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>16</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>0.826765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>16</td>\n",
              "      <td>256</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>0.819889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>16</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.822968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>16</td>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>0.828407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>16</td>\n",
              "      <td>128</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>0.824610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>16</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0.818555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>16</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>0.828715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>16</td>\n",
              "      <td>256</td>\n",
              "      <td>6</td>\n",
              "      <td>64</td>\n",
              "      <td>0.809832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0.812910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>0.817837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>16</td>\n",
              "      <td>128</td>\n",
              "      <td>6</td>\n",
              "      <td>64</td>\n",
              "      <td>0.828920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>32</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.823584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>32</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>0.824815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>32</td>\n",
              "      <td>256</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>0.815476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.817323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>0.828407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>0.822455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>32</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.814450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>32</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>0.826149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>32</td>\n",
              "      <td>256</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>0.825534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.793309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>0.814758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>0.812603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>32</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0.820197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>0.810961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>256</td>\n",
              "      <td>6</td>\n",
              "      <td>64</td>\n",
              "      <td>0.815784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0.826457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>0.815066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>6</td>\n",
              "      <td>64</td>\n",
              "      <td>0.826663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>128</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.813116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>128</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>0.822968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>128</td>\n",
              "      <td>256</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>0.824097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.825944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>0.792282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>0.824199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>128</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.828305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>128</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>0.825226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>128</td>\n",
              "      <td>256</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>0.821736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.819581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>0.823686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>0.825739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>128</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0.809626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>128</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>0.817631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>128</td>\n",
              "      <td>256</td>\n",
              "      <td>6</td>\n",
              "      <td>64</td>\n",
              "      <td>0.822250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0.803161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>0.813937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>6</td>\n",
              "      <td>64</td>\n",
              "      <td>0.826663</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff136205-21ef-4d74-a30a-bd880ec30b3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff136205-21ef-4d74-a30a-bd880ec30b3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff136205-21ef-4d74-a30a-bd880ec30b3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "scores_to_dataframe(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWcYHJt3_wu0"
      },
      "source": [
        "The paramters we choose to tune for the Grid search were batch size, embedding dimensions, number of inputs to the multi head attention and layer size.\n",
        "\n",
        "Other parameters like vocab_size, dropout, batch normalization, optimizer, activation functions, sequence length and l1/l2 were manually tested. \n",
        "\n",
        "- Just like previous models normalizing the data did not help\n",
        "- After trying various sequence length values 600 was the only one that gave a decent accuracy. Any other length would drastically drop the accuracy.\n",
        "\n",
        "From the accuracy scores obtained after grid seach, it can be seen that- even after trying all possible combinations of paramters, the best we could get is a validation accuracy of about 82.8%. Further, applying this tuned model on test data:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()"
      ],
      "metadata": {
        "id": "oYnVlzwCV1JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpfeiQJ2bR5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04130632-1631-48f6-df31-314e6d4f2cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posit  (None, None, 128)        716800    \n",
            " ionalEmbedding)                                                 \n",
            "                                                                 \n",
            " transformer_encoder (Transf  (None, None, 128)        412736    \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,129,665\n",
            "Trainable params: 1,129,665\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "711/711 [==============================] - 12s 14ms/step - loss: 0.4954 - accuracy: 0.7916 - val_loss: 0.4185 - val_accuracy: 0.8175\n",
            "Epoch 2/100\n",
            "711/711 [==============================] - 10s 14ms/step - loss: 0.3688 - accuracy: 0.8534 - val_loss: 0.3926 - val_accuracy: 0.8346\n",
            "Epoch 3/100\n",
            "711/711 [==============================] - 10s 15ms/step - loss: 0.3335 - accuracy: 0.8708 - val_loss: 0.4039 - val_accuracy: 0.8311\n",
            "Epoch 4/100\n",
            "711/711 [==============================] - 10s 15ms/step - loss: 0.3074 - accuracy: 0.8813 - val_loss: 0.4009 - val_accuracy: 0.8311\n",
            "Epoch 5/100\n",
            "711/711 [==============================] - 10s 14ms/step - loss: 0.2961 - accuracy: 0.8884 - val_loss: 0.4164 - val_accuracy: 0.8229\n",
            "Epoch 6/100\n",
            "711/711 [==============================] - 10s 15ms/step - loss: 0.2831 - accuracy: 0.8949 - val_loss: 0.4427 - val_accuracy: 0.8183\n",
            "Epoch 7/100\n",
            "711/711 [==============================] - 10s 14ms/step - loss: 0.2739 - accuracy: 0.8988 - val_loss: 0.4575 - val_accuracy: 0.8247\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f67c0120370>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "vocab_size = 5000\n",
        "sequence_length = 600\n",
        "embed_dim = 128\n",
        "num_heads = 6\n",
        "dense_dim = 64\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "model.fit(X_train_3,y_train, epochs=100,batch_size=16,callbacks=[early_stopping],validation_split=0.3, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmBvX0LriC6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7365efb-5a71-47c1-90dd-c629db7b63d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "218/218 [==============================] - 2s 9ms/step - loss: 0.4599 - accuracy: 0.8268\n",
            "test accuracy:  0.827\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test_3, y_test)\n",
        "print(f\"test accuracy: {test_acc: .3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm_7GLw5bSmP"
      },
      "source": [
        "We could improve the accuracy from 80% to 82%. Although it's the better performing model, it performs only slightly more accurate than our previous models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YCphoVeoZiX"
      },
      "source": [
        "## **Basic ML techniques**\n",
        "\n",
        "Can we predict if the news is fake or not from its source and number of re-tweets?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFk5ZagRvAnI"
      },
      "source": [
        "> KNN Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPMHc31hvDPA"
      },
      "outputs": [],
      "source": [
        "df_knn=df.drop(['title','news_url'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9CjPt1Q0q9W"
      },
      "outputs": [],
      "source": [
        "df_knn = pd.get_dummies(df_knn, drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HshHOwKU0zLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c86b9a-ae07-49fd-925e-5c0929e6ede0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23196, 2442)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "df_knn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbAtlwOR06jM"
      },
      "outputs": [],
      "source": [
        "target='real' \n",
        "predictors = list(df_knn.columns)\n",
        "predictors.remove(target)\n",
        "\n",
        "X_knn = df_knn[predictors].values\n",
        "y_knn = df_knn[target].values\n",
        "\n",
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X_knn, y_knn, test_size=0.30, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5973_Bbz1Fty"
      },
      "outputs": [],
      "source": [
        "quant_features = [0]\n",
        "xform = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scaler', StandardScaler(), quant_features)\n",
        "    ],\n",
        "    remainder='passthrough'  # do nothing with other columns\n",
        ")\n",
        "\n",
        "X_train_knn = xform.fit_transform(X_train_knn)\n",
        "X_test_knn  = xform.transform(X_test_knn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Que8vza1_ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd827fcd-eda2-4631-eb82-a2ac486c77e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy: 0.749\n"
          ]
        }
      ],
      "source": [
        "vals, counts = np.unique(y_train_knn, return_counts=True)\n",
        "target_mode = vals[counts.argmax()]\n",
        "baseline= (y_train_knn==target_mode).mean() \n",
        "print('Baseline Accuracy: {:.3f}'.format(baseline))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8bUH9tZ2HJp"
      },
      "outputs": [],
      "source": [
        "clf = KNeighborsClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pfpgj3W2iCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abbea3c3-4cc5-4cf2-b2b5-362f86a336aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation accuracy: 0.798\n"
          ]
        }
      ],
      "source": [
        "accs = cross_val_score(clf, X_train_knn, y_train_knn, scoring=\"accuracy\", cv=10).mean()\n",
        "print('Cross-validation accuracy: {:.3f}'.format(accs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ht_fBaG5Mbg"
      },
      "outputs": [],
      "source": [
        "cv_accuracy = []\n",
        "ks = range(1,20,2)\n",
        "for k in ks:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    accs = cross_val_score(knn, X_train_knn, y_train_knn, scoring=\"accuracy\", cv=10)\n",
        "    cv_accuracy.append(accs.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6RdlTFk616E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "652b3c00-8b92-4591-ed90-ddaf731e7fdf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(ks, cv_accuracy)\n",
        "plt.title('Cross-validation accuracy by k')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('accuracy');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqY3CHFL7yd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5cf8a10-8e3a-41fe-d6a0-f831d3c07605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation accuracy: 0.809\n"
          ]
        }
      ],
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "clf.fit(X_knn, y_knn)\n",
        "y_pred = clf.predict(X_knn)\n",
        "\n",
        "accs = cross_val_score(clf, X_train_knn, y_train_knn, scoring=\"accuracy\", cv=10).mean()\n",
        "print('Cross-validation accuracy: {:.3f}'.format(accs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unpWQB2C8WlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83dd721d-c956-4092-9bbe-fef2b0c432fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.809\n"
          ]
        }
      ],
      "source": [
        "test_accuracy = clf.score(X_test_knn,y_test_knn)\n",
        "print('Test accuracy: {:.3f}'.format(test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying a basic knn classifier to predict fake news from source of the article and the number of re-tweets gave an accuracy of 80% from the baseline.\n",
        "\n",
        "This model was scaled, transformed and tuned for the value of K to find the best model. It can be seen from the graph that k value of 3 gave the maximum accuracy."
      ],
      "metadata": {
        "id": "JQgyCJ_1Y25O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZLSLFQbnb36"
      },
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qsI64t3oXtv"
      },
      "source": [
        "## **Conclusions**\n",
        "\n",
        "\n",
        "To conclude, we created various models to predict the authenticity of a news article from its title and learned a lot about not only the techniques used but also the dataset, and the possible solutions that can be made in the future. \n",
        "\n",
        "An interesting pattern noticed with this dataset was that it does not work that well with the order of the words. Whenever the \"order\" was used in the dataset like sequence models and positional embeddings- accuracy was affected. \n",
        "\n",
        "One possible reason for this could be because- news article titles contain the fewest possible words to describe the contents of the actual article. In this case, the order of words does not matter as much. \n",
        "\n",
        "Considering the predictor were short sentences, training the models to learn based on word frequency and order- reaching the 83-84 accuracy mark was the best effort made. \n",
        "\n",
        "In the future, this dataset and the conclusions made from this project can also be applied to more complex and advanced deep-learning techniques to get better accuracy and value by integrating those models into real-world systems\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3qsI64t3oXtv"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}